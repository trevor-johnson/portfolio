{"cells":[{"cell_type":"markdown","source":["Team 5 members: Esther (Yuqiao) Chen, Trevor Johnson, Michelle Shen, Yi Zhang"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e066067-43a6-4468-b9a2-ac08b8169cb3"}}},{"cell_type":"markdown","source":["# Introduction"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cba8b85e-6ad5-4988-b22b-a9a9c5f35bab"}}},{"cell_type":"markdown","source":["## Overview\nAir travel is a massive industry with total revenue exceeding $200 billlion each year between 2015 and 2019, and accounts for ~5% of the US's total GDP as of 2020 [1]. Unexpected delays have a large impact on the airlines, the airport, and of course, the passengers, as they can cause significant chain reactions to the downstream scheduling and organization of later flights.\nOur team of data scientists is tasked by a group of airline executives to design a system that is capable of predicting flight departure delays ahead of schedule, so airlines have the opportunity to anticipate, mitigate and reduce delays. As a result, this would increase customer satisfaction and operational efficiency while reducing associated costs.\nThe goal of this system is to be integrated with the existing airline coordination systems to provide early warning to the airline and airport staff so they might be able to mitigate and reduce the predicted delay. Customers will explicitly not have access to these prediction results.\nTherefore the airline executives will be the primary stakeholders, and the airline/airport staff will be the secondary stakeholders of this project.\n\nConcretely, the objective of this project is to create a predictive machine learning classifier to give adequate advance notice to airline and airport staff of potential flight delays. Our goal is to predict flight departure delay (i.e. whether a flight will be delayed by more than 15 minutes from its scheduled time) from major U.S. airports, 2 hours before the scheduled departure time.\nWith the 2 hours of lead time, this would allow the airlines and the airport to potentially make necessary adjustments to mitigate the delay and have the flight back on-track, and at least fairly compensate the passengers. Because our goal is to advise airline executives and give them an adequate heads up of potential flight delay, we tuned our models to prioritize capturing all the situations where there is a potential flight delay. We erred on the side of overpredicting delays as will be shown through our choice to use F2 score."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"17bb3087-5200-43ad-b027-76e6c6d59479"}}},{"cell_type":"markdown","source":["## Related Work\n\nPredicting flight delays is an active area of research. In recent years, with the rise of Machine Learning, various techniques have been applied to this problem, such as Neural Networks[2], Linear Regression [3], SVM [4], and Gradient Boosted Decision Trees [4].\nRecent state-of-the-art research papers all seem to have taken a rather broad approach to data gathering. They have taken signals from multiple channels for training and prediction, including not only the flight and weather information, but real-time air traffic messages [2], airport traffic data [4], and so on."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9cba72da-f4e5-4e8f-9cb8-bab9747d04d6"}}},{"cell_type":"markdown","source":["## Performance Evaluation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d5680813-b426-43d0-8539-88fa05bc4d95"}}},{"cell_type":"markdown","source":["### Metrics\nThe response variable, whether or not a flight is delayed, was imbalanced with roughly 23% of flights being classified as delayed. Because of this, using model **accuracy** alone is not the best metric to judge model performance. However, we do have a number of other metrics available [5].\n\nThe most common metric for imbalanced data is the **F-score**. With the definition of \"no delay\" being a _negative_ result, and \"delay\" being a _positive_ result, a prediction can fall into 4 categories:\n- True Positive: The flight was predicted as delayed and is actually delayed\n- True Negative: The flight was predicted as not delayed and is actually not delayed\n- False Positive: The flight was predicted as delayed but is actually not delayed\n- False Negative: The flight was predicted as not delayed but is actually delayed\n\nWe can then define **Precision**, **Recall**, and the ordinary **F1-score** as follows:\n- Precision = \\# True Positive / (\\# True Positive + \\# False Positive)\n- Recall = \\# True Positive / (\\#True Positive + \\# False Negative)\n- F1-score = 2 * Precision * Recall / (Precision + Recall)\n\nIn this case, a False Positive outcome would likely cause airport and airline staff to waste resources and spend time trying to make sure the given flight is on time for departure when it would have been on time without intervention. \n\nHowever, a False Negative outcome would completely defeat the purpose of the advance notice system we are trying to create, since the purpose of our prediction algorithm is alerting airlines to delayed flights they otherwise would not have anticipated. If a false negative occurs and the predictive algorithm reports an on-time flight when the flight is delayed in reality, internal staff would be unprepared as the flight would recieve less attention for correction, leading to downstream additional connection delays, and airlines could lose trust with customers if they do not take proper measures to alert the customers to the delay.\n\nGiven these consequences, a False Negative has arguably higher cost than a False Positive, in other words, recall is more important here than precision.\n\nQualitatively, we assume recall is twice as important as precision, which gives rise to our final evaluation metric of **F2-score** [6]:\n$$\nF_2 = \\frac{5 \\cdot Precision \\cdot Recall}{4 \\cdot Precision + Recall}\n$$\n\nThere are other notable metrics that work well for imbalanced data, such as measuring the area under an ROC curve or using a precision-recall curve. Analyzing these scores was helpful to think about probability thresholds. But because of our business problem, we chose to focus on F2 score as the primary objective."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e96f46d-303a-44e0-9351-94725adb6945"}}},{"cell_type":"markdown","source":["### Cross Validation Evaluation\nTo evaluate the success criteria (i.e. F-score) of the models, we split the data into a training set and a test set so that the model was evaluated on unseen data for a fair score. Additionally, since the models require hyper-parameter tuning via cross validation, for each fold, we used a test set (a.k.a. development set) of data to find the best combination of hyper-parameters.\n\nDue to the Flights data containing a time series component (i.e. with a natural time progression), we split the data into **training** and **test** sets by blocks for cross validation on a rolling basis.\n\nSpecifically, we split the Flights data from 2015 to 2019, into quarters (where each quarter is 3 months since the start of the year), resulting in 20 quarters of data.\nWe used a 5-fold rolling cross validation for hyperparameter tuning, and we reserved a separate hold-out set for final evaluation [7].\n\nThe exact data split was as follows:\n\n|Fold |    Training Set    | Development (Test) Set | Test Set |\n|-----|--------------------|----------|----------|\n|1    | 2015 Q1 to 2017 Q3 | 2017 Q4  | --- |\n|2    | 2015 Q2 to 2017 Q4 | 2018 Q1  | --- |\n|3    | 2015 Q3 to 2018 Q1 | 2018 Q2  | --- |\n|4    | 2015 Q4 to 2018 Q2 | 2018 Q3  | --- |\n|5    | 2016 Q1 to 2018 Q3 | 2018 Q4  | --- |\n|Final evaluation | 2015 Q1 to 2018 Q4 | --- |2019 Q1 to 2019 Q4|"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06e30ca8-4540-42dc-a3f1-df01a91de29b"}}},{"cell_type":"markdown","source":["# Data Acquisition and Preprocessing"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"87ba9c41-8c08-4429-94e5-943558e6197a"}}},{"cell_type":"markdown","source":["## Data Overview\n\nThe primary dataset that we used was the Flights dataset. This data is a subset of the passenger flight's on-time performance data taken from the TranStats data collection available from the U.S. Department of Transportation (DOT). A data dictionary for this dataset can be found [here](https://www.transtats.bts.gov/Fields.asp?gnoyr_VQ=FGJ). The dataset is in the format of parquet files, and there are total 31,746,841 records on flights departing from all major U.S. airports for the January 2015 to December 2019 timeframe. The following are some of the key fields in this dataset that pertain to flying performance: origin and destination of flights, departure and arrival time, carrier airline information, etc.\n\nThe second dataset that we had available for predicting flight delays was the Weather dataset. This dataset was pre-downloaded from the National Oceanic and Atmospheric Administration repository. A data dictionary for this dataset can be found [here](https://docs.google.com/spreadsheets/d/1v0P34NlQKrvXGCACKDeqgxpDwmj3HxaleUiTrY7VRn0/edit#gid=0). The following are some notable features found in this dataset: wind observation, sky condition, visibility observation, etc. There are 630,904,436 records in this dataset.\n\nThe third dataset we used was the Airport Information dataset from [OpenFlights.org](https://openflights.org/data.html). This dataset contains over 10,000 ariports spanning the globe. Each entry contains the following information: airport name, IATA code, timezone, longitude and latitude information, ..., etc. This dataset helped us to join our Flights table with Weather stations together."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c0e1e48-331a-4508-a19f-f8276abac2c3"}}},{"cell_type":"markdown","source":["## EDA\n\nIn order to understand the datasets and choose what features to keep and drop for our final joined dataset, we completed EDA on all three provided datasets. We plotted graphs to visualize feature distributions and identify relevant features for feature engineering."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8aa174e6-777a-4fdd-aa23-90bf40cae0f2"}}},{"cell_type":"markdown","source":["### Flights Dataset\n\nBased on the preliminary exploration of the attributes available in this flight dataset, we identified 6 main categories out of 109 attributes, summarized as the following: <br /><br />\n\n1. **Time Related**: YEAR, QUARTER, MONTH, DEP_TIME, CRS_DEP_TIME, CRS_ELAPSED_TIME, ... etc.\n2. **Airport/Geographincal Information**: ORIGIN_AIRPORT_ID, DEST_AIRPORT_ID, ORIGIN_CITY_NAME, ORIGIN_STATE_ABR, ...etc.\n3. **Carriers Information**: OP_UNIQUE_CARRIER, OP_CARRIER_AIRLINE_ID, OP_CARRIER, OP_CARRIER_FL_NUM\n4. **Delay Information**: DEP_DELAY, DEP_DELAY_GROUP, WEATHER_DELAY, SECURITY_DELAY, ...etc.\n5. **Aircraft/Taxi Information**: WHEELS_OFF, WHEELS_ON, TAXI_IN, TAXI_OUT, ...etc.\n6. **Trip Information**: DISTANCE, DIVERTED, CANCELLED, ...etc.\n\nAmong the Delay Information, we figured out that DEP_DELAY could be used as our outcome variable.\n\n**EDA on Full Flight Dataset**\n\n***Several key variables of the dataset are listed below:***\n\n|Property|Data|\n|-----------|----|\n|Total flight count|63493682|\n|Total Flight delayed by 15 minutes or more|11387082|\n|% delayed flight|17.93%|\n|% on-time flight|80.56%|\n|% cancelled flight|1.54%|\n|% diverted flight|0.25%|\n\n\n***Class Imbalance for Departure Delays***\n\nThe total number of flights delayed by more than 15 minutes was 11387082, which is about 17.93% of the total number of flights, and the total proportion of on-time flights was 80.56%. This showed a class imbalance which could lead to a prediction bias towards the \"on-time flight\".\n\nThe figure below shows the DEP_DELAY_GROUP which groups delay times by 15 minute intervals. As expected, the departure delay group distribution was skewed to the right since the majority of flights were delayed less than 15 minutes.\n\n![distribution_delays](files/shared_uploads/yuqiaochen@berkeley.edu/distribution_delays.png)\n\n***Factors that can cause flight delay***  <br />\n\n**1. Airline Carrier Factor**\n\nBecause different airline carriers may have different delay rates, we calculated the percentage of delays over the total flights for each airline carrier, and also plotted a graph to show the top 10 airline carriers with the highest percentage of delays.\n\n![airline_delay](files/shared_uploads/yuqiaochen@berkeley.edu/airline_delay.png)\n\nFrom the graph above, we observed that JetBlue(B6), Frontier Airlines(F9), Virgin America(VX), Southwest Airlines(WN), Spirit Airlines(NK)... etc had the highest delay rates, and the airline carriers were indeed associated with flight delays.\n\n**2. Time-Related Factor** \n\nWe believed that seasonality and time-related factors such as hour, day of the week, and holidays had an impact on flight delays. Therefore, we explored a few time features in relation to the flight departure delay rates and plotted histogram graphs of the distributions.\n\n\n![time_related_factors_vs_delays](files/shared_uploads/yuqiaochen@berkeley.edu/time_related_factors_vs_delays.png)\n\nWe observed the following results:\n- Day of Week: Monday, Thursday and Friday have more departure delays than other days of the week\n- Hour of Day: Late afternoon and early morning have more departure delays than other time of the day\n- Month of Year: June, July, August and December have more departure delays than other months of the year\n- Quarter of Year: Second quarter of the year has more departure delays than other quarters of the year\n\n**3. Flight Distance Factor** \n\n*From PHASE 1 (Using the data from the first quarter of 2015)*: We also thought that there might be a relationship between delay and flight distance, so we plotted a histogram of average delay by flight distance group which aggregates the average departure delay by distance group with 250 miles intervals. \n\n![distance_delays](files/shared_uploads/yuqiaochen@berkeley.edu/distance_delays.png)\n\nWe observed from the plotted graph: Longer distance flights were delayed with an average of 20-25 minutes. For shorter distance flights, the average delay time was less than 15 minutes.\n\n*From PHASE 2 (Using full data)*: By using the full flight dataset, we plotted this graph again and observed that the average departure delay and flight distance had a nearly uniform distribution in the histogram.\n\n![distance_graph](files/shared_uploads/yuqiaochen@berkeley.edu/distance_graph.png)\n\n\n**3. Airport Factor** \n\n![airport_delay](files/shared_uploads/yuqiaochen@berkeley.edu/airport_delay.png)\n\nThe top 5 airports with the most number of delays are McKinney National Airport (TKI), Youngstown-Warren Regional Airport (YNG), Wendover Airport (ENV), Wilmington Airport (ILG) and Southwest Oregon Regional Airport (OTH). This did not make much sense because these are not the busiest and most heavily traveled connection hubs in the United States as far as we know. The busiest connection hub airports such as Dallas/Fort Worth (DFW), Chicago–O'Hare (ORD), Los Angeles (LAX), and New York–Kennedy (JFK) were not found in the top 10 aiports with highest percentage of delays."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9f8b310-9d3c-4487-b1ee-598ecb2d1ac9"}}},{"cell_type":"markdown","source":["### OpenFlights Dataset\n\n```NOTES: \nIATA code is a three-letter geocode designating many airports and metropolitan areas around the world.\nFlights dataset uses IATA code as ORIGIN/DEST.\n```\n\nBy comparing OpenFlights dataset and the Flights dataset, we found the following missing airports:\n\n![missing_airports_from_openflight](files/shared_uploads/yuqiaochen@berkeley.edu/missing_airports_from_openflight.png)\n\nWe manually filled in information (IATA Code, Timezone, Longitude/Latitude) for these missing airports.\n\n```\nopen_flights2 = spark.createDataFrame(pd.DataFrame({\n    'iata': ['XWA', 'EAR', 'TKI', 'IFP'], \n    'icao': ['KXWA', 'KEAR', 'KTKI', 'KIFP'], \n    'lat': [48.2578135, 40.7274925, 33.1775399, 35.16558], \n    'lng': [-103.7418471, -99.0122646, -96.5926444, -114.557093], \n    'tz_db_time_zone': ['America/Chicago', 'America/Chicago', 'America/Chicago', 'America/Phoenix']\n}))\n```\n\nThen we were able to find the closest single weather station for each airport based on a longitude and latitude join using the weather stations."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03a4f651-6eec-44af-a35a-706576ce4a49"}}},{"cell_type":"markdown","source":["### Weather Dataset\n\n**Inital EDA of Weather Dataset** <br><br>\nThe weather dataset consists of 630904436 records with 177 variables. <br>\nFor the initial EDA of the weather dataset, we focused first on understanding the variable names and contents. Secondly, we focused on narrowing down the variables due to the size of the weather dataset. <br>\nUsing the data dictionary, we discovered that:\n1. Many variables are amalgamations of several comma-separated, related features\n> e.g. The record for `WND` could be `190,1,N,0015,1`, which translates to 5 `WND` subfeatures represented in one column, one subfeature for every comma-separated value:  <br> \n> `Wind direction angle: 190 angular degrees` <br> \n> `Wind direction quality code: 1` <br> \n> `Wind direction type code: N (normal)` <br> \n> `Wind direction speed rate: 0015 meters per second` <br> \n> `Wind direction speed quality code: 1 = Passed all quality control checks` <br> \n\n2. Some variables are different instances of the same type of observation\n> e.g. `AA1` and `AA2` both represent episodes of liquid precipitation.\n\n**Preliminarily reducing number of variables**\n\nBecause there were 177 variables in the Weather Data, a reduction of a single column of variables could significantly improve the runtime of our join. Alternatively, *not reducing* the number of variables before joining could cause our join to stagnate. As a result, due to the large number of variables we noted were missing data, we explored the percentage of each column consisting of missing values in order to discard variables with a high percentage (>50%) of missing values. In addition, our research led us to hypothesize that inclement weather could be a major cause of flight delays [8]. Therefore, we retained some variables despite their possessing a high percentage of missing values if their descriptions revealed them to be indicators that could capture precipitation, fog, or low visibility.\n\nTo do this, we completed the following steps:\n\n1. Subsetted the weather dataset to a (pseudo)random sample of 30% of total records (189257881 records); we made the assumption that this sample's data distribution was representative of the larger dataset\n2. Obtained the number of records for each variable that contained \"missing\" values (null, empty string, NA, or blank entries)\n3. Divided the number of null values by the number of total records in the subset and converted it to a percentage\n4. Filtered out variables for which 50% or less of their records were filled; kept the remaining 20 variables\n5. Manually selected for some fog- and precipitation-related variables based on the initial data dictionary exploration and added them back into the group we planned to keep despite the low percentage of data that they represent\n\nThe table below shows the variables we kept when executing the join. For those variables that were instances of multiple observations of the same weather occurrence, we usually kept only the first of those observations because most were so sparsely populated that it didn't make sense to keep multiples.\n\n|Variable Name|Percentage of Data Filled|\n|-------------------|-----|\n|STATION|99.38|\n|SOURCE|87.01|\n|DATE|100.00|\n|LATITUDE|100.00|\n|LONGITUDE|100.00|\n|ELEVATION|100.00|\n|NAME|99.24|\n|REPORT_TYPE|100.00|\n|CALL_SIGN|100.00|\n|QUALITY_CONTROL|100.00|\n|WND|100.00|\n|CIG|100.00|\n|VIS|100.00|\n|TMP|100.00|\n|DEW|100.00|\n|SLP|100.00|\n|GA1|50.67|\n|GF1|57.27|\n|MA1|78.95|\n|AA1|23.32|\n|AA2|3.15|\n|AJ1|1.16|\n|AL1|0.16|\n|AN1|0.12|\n|AO1|12.74|\n|AU1|4.74|\n|AT1|0.32|\n\n\nAfter splitting variables, some EDA was performed to look at whether the subvariables might be impactful throughout the year based on a random sample of 10% of the Weather dataset. The following plot shows the distribution of the average amount of liquid precipitation in millimeters over the 52 weeks of the year. This distribution suggests that if liquid precipitation is impactful to flight delays, it is relatively evenly represented in the data, assuming that the 10% sample was representative of the distribution of the full Weather dataset.\n\n![weather_1](files/shared_uploads/yuqiaochen@berkeley.edu/weather_1.png)\n\n\nThe following plot shows the distribution of the average amount of snow and ice on the ground in centimeters over the 52 weeks of the year. This distribution suggests that if snow and ice on the ground is impactful to flight delays, the months from late October to April might be significant. Therefore, some feature engineering was necessary.\n\n![weather_2](files/shared_uploads/yuqiaochen@berkeley.edu/weather_2.png)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1fa9ec3-1fd2-4612-869f-85e39b14ac34"}}},{"cell_type":"markdown","source":["For more on EDA, please see the following notebooks:\n* Phase 1-3 cumulative summary of EDA [here](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/2272311596985675/command/1038316885898617)\n* Phase 1: [Flight](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/1038316885899293/command/1038316885899295) and [Weather](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/1038316885902791/command/1038316885902793)\n* Phase 2: [Flight](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/3816960191362867/command/3816960191362868) and [Weather](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/3816960191359511/command/3816960191359512)\n* Weather EDA plots [here](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/1858507102438204/command/1858507102438207)\n* Data dictionary exploration: [Flight](https://docs.google.com/spreadsheets/d/1VKBW1Grj1eMToz9rRHRpO9RxljPsJ3Fs/edit?usp=sharing&ouid=114707694286470350740&rtpof=true&sd=true) and [Weather](https://docs.google.com/spreadsheets/d/1xthRtOjC5-kV0LMIRa_grkxZ6sLBlpqDpNwygOH_jlA/edit?usp=sharing)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c2d01d8-ff02-431e-b67e-28d3b3f26a46"}}},{"cell_type":"markdown","source":["## Joining the Datasets\n\nWe diagrammed our join process in the figure below:\n\nThis first diagram shows how we handled missing airport data and generated a mapping table containing the closest single weather station for each airport.\n\n![Joining_Process_1](files/shared_uploads/yuqiaochen@berkeley.edu/Joining_Process_1.png)\n\nThis second diagram shows how we performed the joins to get our final dataset.\n\n![Joining_Process_2](files/shared_uploads/yuqiaochen@berkeley.edu/Joining_Process_2.png)\n\nWe performed the following processes to generate the mapping table: \n1. Manually filled in data (ICAO, IATA, LONGITUDE, LATITUDE) for the missing airports that were not in the OpenFlights dataset. \n2. Joined the enriched OpenFlights dataset that contained all airport codes and lon/lat info with the Weather table on the closest longitude and latitude, to get the mapping table containing the closest single weather station for each airport.\n\nWe performed two final joins:\n1. Joining the mapping table containing the closest single weather station for each airport with the Weather table on StationID to get an enriched Weather table with airport codes.\n2. Joining the enriched Weather table with the Flights dataset on both `IATA` = `ORIGIN/DEST` and the nearest weather `Timestamp` to two hours prior to flight departure time.\n\nTo ease the computation on the time dimension, we rounded the weather measurements to top of the hour and we took 3 hours before the flight time, and also rounded to top of the hour, and performed a join on that. Essentially for each flight, we first took all the available weather measurements at both origin and destination between 2 and 3 hours before scheduled departure, then simply took the earilest of the measurements per location. Even though this introduced some variety on the time of the weather measurement, it sped up the join computation significantly, so we felt it was worth it.\n\nA demonstration of our preliminary join exploration is documented in [this linked workbook](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/1038316885899254/command/1038316885899260).\nOur final process to join the data sets together is performed in [this workbook](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/3816960191354743/command/3816960191362643)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7f19d11-049f-4c28-acda-5fdd6660b8fe"}}},{"cell_type":"markdown","source":["## Splitting the Weather Dataset\n\nIn order to give weather variables containing more than one subvariable meaning and to make them usable as features, we used the provided documentation to help generate a series of dictionaries allowing us to store information about each subvariable. A limitation of this method was the initial investment in hardcoding data into the dictionaries, but the result was modularized dictionaries and functions that could be easily updated if we wanted to change the way we typecasted our variables. After splitting, quality code subvariables describing the quality of another subvariable under the same supervariable were dropped, as measurements were generally of good quality. In addition, several categorical variables that were found to contain all null values were also dropped. Numerical variables were cast as doubles. Scaling factors in numerical variables were also accounted for so that those subfeatures with scale factors were automatically scaled back to a scale factor of 1 during the generation of new column variables. \n\nThe following is an example of variables before and after splitting:<br><br>\n_Before (supervariable):_ <br>\n|'WND_WTHR'|\n|-------------------|\n|'190,1,N,0015,1'|\n\n_After (subvariable):_ <br>\n|WND_WTHR_direction_angle|WND_WTHR_direction_quality_code|WND_WTHR_type_code|WND_WTHR_speed_rate|WND_WTHR_speed_quality_code|\n|---------------|-----------------|----------------|----------------|-------------|\n|190.0|'1'|'N'|15.0|'1'|\n\nFor more on weather variable splitting functions, please see [this notebook](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/4070574709969671/command/4070574709969672)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"45ad7641-6fb4-4e1f-a0a2-fbbdb388a4a5"}}},{"cell_type":"markdown","source":["# Feature Engineering"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bfb4fbe1-e5ab-4d79-8b00-873bfa9c5071"}}},{"cell_type":"markdown","source":["## Dropped Variables\n\n##### 1. Dropping duplicates\n\n![duplicate_flights](files/shared_uploads/yuqiaochen@berkeley.edu/duplicate_flights.png)\n\nFrom our EDA, we found that there were 50% duplicated records (31756841 records out of 63493682 records) in the Flights dataset, so our first step was to drop duplicates from that dataset and to keep only unique records.\n\n##### 2. Dropping cancelled flights\n![cancel_flight](files/shared_uploads/yuqiaochen@berkeley.edu/cancel_flight.png)\n\nAfter checking the documentation of flight dataset, we knew that cancelled flights could result from the following reasons: Carrier, Weather, National Air System or Security. By doing EDA, we found that there were 1.5% flights that had been cancelled and there were 1.5% records had the DEP_DEL15 as NULL, which meant that these records with NULL as DEP_DEL15 were the cancelled flights. Since the percentage of cancelled flight was relatively small, we considered cancelled flights a different set of circumstances than flight delays. As a result, we designated DEP_DEL15 as our target variable and we removed the cancelled flight records from the Flights table.\n\n##### 3. Dropping diverted flights\n![divert_flight](files/shared_uploads/yuqiaochen@berkeley.edu/divert_flight.png)\n\n![flight_table_info](files/shared_uploads/yuqiaochen@berkeley.edu/flight_table_info.png)\n\nFrom the documentation of Flights dataset, we know that a diverted flight is a flight that is required to land at a destination other than the original scheduled destination for reasons beyond the control of the pilot/company. We omitted all diverted flights from our Flights table since the percentage of diverted flights over total records in the table was relatively small (0.25%) and was not relevant to our analysis."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b8d4a9d-fd22-44ee-9d86-d3eeefc75812"}}},{"cell_type":"markdown","source":["## Duplicates and Outliers\nTo detect outliers, we created boxplots to explore variable distributions and more easily detect extreme values. To detect missing values, we performed group by operations to count the number of times missing values occurred. Additionally, several of the weather features noted above had values of \"999\" (or a similar string of one to six 9's) which meant that the particular observation was missing, and should not be treated as an outlier. We treated these \"999\" values the same way we treated the missing values in the dataset.\n\nWe handled missing values and outliers differently depending on the model we used:\n- For our __logistic regression__ model, we had to make sure to identify missing values and extreme outliers.   \n  - Categorical features: For variables with several missing values, we imputed a value called \"missing\" so these observations could still be used in our model.\n  - Continuous features: For variables with several missing values, we performed median imputation. The median value for each observation came from the median value from the corresponding airport and month.\n  - For both categorical and continuous variables with very few missing values, we removed these observations from the dataset. Some weather features had way too many missing values and had to be removed from the dataset altogether. \n  - We removed several extreme values from the training data to avoid skewing the logistic regression coefficients. Removing these extreme outliers is important for a logistic regression because these points have high leverage and will cause the coefficients to be more biased.\n\n- For our __tree based__ random forest and XGBoost models, missing and extreme values are slightly less important. However, for simplicity we ended up starting with a training dataset similar to the one used for logistic regression with several missing values imputed and outliers removed. Only a few features remaining had missing values, for which we performed median or 0 imputation before modeling. \n  - For missing data, tree models can use \"surrogate splits\" which default to a different variable to control the split if a given observation is missing from the primary variable. Additinally, tree models are not very biased when dealing with extreme values due to how feature are binned. For these reasons we were less worried about missing values and outliers in these models.\n\nHere are a few boxplots showing the distribution of some key variables that show outlier values:\n\nWeather Dataset:\n\n<img src=\"files/shared_uploads/trevorj@berkeley.edu/snow_boxplot.png\" width=\"30%\">\n\n<br><br>\n\n<img src=\"files/shared_uploads/trevorj@berkeley.edu/precipitation_boxplot.png\" width=\"30%\">\n\n\n\n\n\nFlight Dataset:\n\n![flight_boxplots](files/shared_uploads/yuqiaochen@berkeley.edu/flight_boxplots.png)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"830bfd19-7cdf-4498-bef5-261f87620d43"}}},{"cell_type":"markdown","source":["## Data Imputation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c73b6eb1-bc50-4473-bfb3-e6f2231e3717"}}},{"cell_type":"markdown","source":["### Flight Datasets Imputation\nTo deal with nulls in the flight dataset, we have implemented an impute_null_numeric function to impute numerical missing data. It takes in a list of numeric features that we want to impute null values for and also the method that we want to use to do the imputation (it can either be median or mean), and then it groups the records by year, month and airport to do the calculation. By calling this function, we could more systematically deal with null value removal."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab499f37-5879-44e2-8ed0-953798065c2a"}}},{"cell_type":"markdown","source":["### Weather Dataset Imputation\nFor numerical variables, we used arbitrary value imputation [9] on numerical missing data, where a specific value was selected to represent missing data, as we suspected that missing weather values were missing not at random; therefore mean and median imputation would make less sense. The values we chose to impute for numerical variables can be found in [this spreadsheet](https://docs.google.com/spreadsheets/d/1xthRtOjC5-kV0LMIRa_grkxZ6sLBlpqDpNwygOH_jlA/edit#gid=2144784504). We recognize that this arbitrary value imputation can drastically change the distribution of variables. Categorical variables were encoded into individual binary variables. \n\n<br>**Imputation of numerical variables:** <br>\n_Before:_ <br>\n|WND_WTHR_speed_rate|\n|-------------------|\n|null|\n|null|\n|15.0|\n\n_After:_ <br>\n|WND_WTHR_speed_rate|\n|-------------------|\n|0.0|\n|0.0|\n|15.0|\n\n**Categorical variable casting to binary:** <br>\n_Before:_ <br>\n|Other columns|WND_WTHR_type_code|\n|------------------|------------------|\n|Other columns|'N'|\n|Other columns|'R'|\n|Other columns|'Q'|\n\n_After:_ <br>\n|Other columns|WND_WTHR_type_code_N|WND_WTHR_type_code_R|WND_WTHR_type_code_Q|\n|------------------|------------------|------------------|------------------|\n|Other columns|1|0|0|\n|Other columns|0|1|0|\n|Other columns|0|0|1|\n\nFor more on weather variable imputation functions development, please see [this notebook](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/4070574709969671/command/4070574709969672)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b069de65-1643-4768-9027-398e112a7f40"}}},{"cell_type":"markdown","source":["## Derived Features\n\n##### 1. Extraction of local departure hour value from CRS_DEP_TIME\nIntuitively, we felt that the departure time of flights might be in relation to the flight delays, therefore we decided to extract out the local departure hour value from CRS_DEP_TIME variable.\n\n![hour_vs_delays_phase2](files/shared_uploads/yuqiaochen@berkeley.edu/hour_delays.png)\n\n##### 2. Holiday Indicator\n\nSince we noticed in EDA that there were some peaks around the holiday seasons, we included a holiday indicator, which indicates if a date is a public holiday such as Independence Day, Thanksgiving, Christmas, ...etc. We labeled the holiday itself as '1 = holiday', the days on either side of it as '2 = nearby holiday', and days that were not holidays as '0 = not a holiday', since we wanted to capture these patterns in our model.\n\nWe created a table containing all holidays we will consider from 2015 to 2019:\n\n![holiday_table](files/shared_uploads/yuqiaochen@berkeley.edu/Holidays_2015_2019.png)\n\nAnd a list of dates that were near holidays:\n\n![near_holiday_dates](files/shared_uploads/yuqiaochen@berkeley.edu/near_holiday_dates.png)\n\nWe also plotted a graph showing the delay rates for these 3 categories we created (Not a Holiday/Holiday/Near Holiday)\n\n![holiday_graph](files/shared_uploads/yuqiaochen@berkeley.edu/holiday_graph.png)\n\n##### 3. Previous Flight Delay Indicator & Other Related Features\n\n- Previous Flight Delay 15 mins or more (**Previous_Flight_Delay_15**)\n- Enough Time between Estimate Arrival Time of Previous-Flight and Planned Departure Time of Current Flight (**Enough_Time_Btwn_Estimate_Arrival_and_Planned_Dep**)\n- Poor Scheduling Indicator (**Poor_Schedule**)\n\nWe believed that flight delays can have a cumulative effect, meaning if there is any flight being delayed, later flights can be impacted as well, so we decided to create new features relating flights to previous flights.\nWe generated a variable to track if the previous flight was delayed or not, and we calculated the time between the estimated arrival time of the previous flight and the planned departure time of the current flight to see if there was enough time to prepare for the next flight. \nBesides that, we also found there were some flights for which the Scheduled Arrival Time of Previous-Flight was later than the Scheduled Departure Time of Current-Flight. For this kind of poor scheduling of flights, the delay rate was 87.42%, so we also added a poor scheduling indicator to capture these flights.\n\n\nFor more on derived features, please see [this notebook](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/1898361324249652/command/4070574709970056)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08b562f1-5794-43d7-8589-26024dfe4914"}}},{"cell_type":"markdown","source":["## End-to-end Pipeline\n\nIn summary, we performed the following steps from the raw data to have it ready for modeling\n1. Read in raw data\n2. Selected a subset of raw columns that we thought would be useful\n3. Removed duplicated, cancelled, and diverted flights\n4. Identified the closest weather station to each airport and build an airport/station mapping table\n5. Used the mapping table to join weather data at origin as well as destination airport onto the flight data at 2 hours before the flight's scheduled departure\n6. **Check point 1**: saved the joined data to blob storage\n7. Expanded the weather data into multiple columns, scale, and impute them as described above\n8. Added derived features as described above\n9. One-hot encoded a number of categorical features with manual defaults\n10. Removed the outliers as described above\n11. **Check point 2**: saved the processed data to blob storage\n\nAfter check point 2, the data was ready to be modeled. The code for this process is captured in [this notebook](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/1898361324238584/command/1898361324238594).\n\nAdditionally, many of our data engineering processes are captured in self-contained functions that were reused in [this notebook](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/1898361324234161/command/1898361324234162)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"62695ed6-fd3a-4c7c-a48a-8e7df16289d6"}}},{"cell_type":"markdown","source":["# The Models\nAfter feature engineering work was completed, tackled the prediction problem with Machine Learning models. In this section, we discuss the models we explored."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a81440f2-2674-468f-a985-9565b19b8299"}}},{"cell_type":"markdown","source":["## Logistic Regression Models\n\nBecause predicting flight delay is a binary classification problem, we used Logistic Regression as a baseline model.\n\nLogistic Regression essentially assumes a generalized linear relationship between the features and the classification label, where it uses a sigmoid function on top of a linear combination of features, specifically:\n$$\np(x) = \\frac{1}{1 + e^{-\\beta x}} = \\frac{e^{\\beta x}}{e^{\\beta x} + 1}\n$$\n\nwhere \\\\(p(x)\\\\) is the predicted label of a data point with feature vector \\\\(x\\\\), and \\\\(\\beta\\\\) is the coefficients vector. The sigmoid function serves to map a predicted value into a value between 0 and 1.\n\nTo refine the Logistic Regression Model, we did the following:\n- Added in the categorical and derived features we prepared from our feature engineering process\n- Performed more thorough PCA and determined a more optimal value\n- Performed more thorough hyper-parameter tuning on the model, e.g. regularization parameters\n\nWe expected this fine-tuning would help improve the performance of the Logistic Regression model, especially its recall and F-2 score, as its the primary performance metric."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6867f243-68f0-41da-8734-ae4c8bc7195d"}}},{"cell_type":"markdown","source":["## Decision Tree Models\n\nIn addition to the logistic regression model, we also built a gradient boosted decision tree using XGBoost as well as a Random Forest Classifier model. We used these three models in our final ensemble model to make the final prediction. \n\nBefore jumping into modeling, we created variable importance plots to see how often our decision trees decided to use particular variables in the spilts. We included every single available feature into a random forest model, then observed the top 10% of most used features. The plot below shows the top 10% most used features in this model. After observing this, we were better informed as to which variables to make sure to include in our decision tree models. \n\n<img src=\"files/shared_uploads/trevorj@berkeley.edu/Tree_var_imp.png\" width=\"35%\">\n\n<br>\n\nWe similarly did this for our XGBoost model. The results were similar, but not exactly the same. The XGBoost model gave very high importance to both the origin and destination airlines, whereas the random forest classifier gave less weight to these features. However it was clear to see that many of the most important features in the random forest model were also given high importance in XGBoost. We also found that several high-importance features were some of our derived features.\n\n<img src=\"files/shared_uploads/trevorj@berkeley.edu/boosting_varimp.png\" width=\"35%\">\n\n\nBecause random forest and XGBoost both do implicit variable selection, it's not imperative to limit the number of features in the final model. Using the variable importance plots was helpful to make sure we included the most important features in our tree models. In the end, we decided to keep about 60 features in our final tree models to keep them parsimonious. \n\nNext, we performed hyperparameter tuning using 5-fold cross validation to test out different combinations of hyperparameters. \n\nSome hyperparameter tuning for the XGBoost model can be found in [this notebook](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/1858507102378953/command/1858507102378955). After hyperparameter tuning, we found the following hyperparameters to yield a maximum F2 score for the XGBoost model:\n- Max Depth: 10\n- Number of Trees: 60\n- Learning Rate: 0.1\n- Colsample by Tree: .8\n- Gamma = 0.05\n- Alpha = 0\n- Lambda = .1\n\nSome hyperparameter tuning for the Random Forest model can be found in [this notebook](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/1858507102383849/command/1858507102385645). The following hyperparameters yielded the maximum F2 score for the Random Forest model:\n- Max Bins: 370\n- Max Depth: 12\n- Number of Trees: 50\n- Feature Subset Strategy: 'sqrt'\n- Subsampling Rate: 0.8\n\nAs explained earlier, selecting the optimal hyperparameters was time consuming given the large training dataset and limit cluster resources. Thus, we used 25% of the dataset for hyperparameter tuning, and fit 1 final model on the full training dataset. \n\nUsing this method, the XGBoost model took roughly 30-45 minutes to evaluate one set of hyperparameters through 5-fold cross validation. XGBoost then took about 45 minutes to fit the full training dataset. We created a function to clean the raw data and make predictions with XGBoost. This function generally takes about 7 minutes to evaluate on the test dataset. \n\nThe Random Forest model took roughly 60 minutes to evaluate one set of hyperparameters through 5-fold cross validation because I was testing out bigger and more trees. Then it took 15 minutes to fit the full training dataset. Like XGBoost, we created a specific function for data cleaning and making predictions for the random forest. This function takes roughtly 3 minutes to evaluate on the test data. \n\nThe random forest model works by running each observation through the 50 decision trees, and using the majority predicted class as the final predicted probability. \n\n![RF_diagram](files/shared_uploads/trevorj@berkeley.edu/random_forest_diagram.png)\n\nXGBoost on the other hand works by building iterative trees. It begins with the average log odds of the predicted class of the overall dataset. Then creates a single decision tree to try and predict the residuals of each observation. Then it takes those predicted residuals, multiplies each by a learning rate parameter, and adds this value to the overall average log odds prediction. Next, the model builds another decision tree to predict the updated residuals. Then those new residuals are once again multiplied by the learning rate and added to the running chain of predicted log odds probabilities. This continues until early stopping criteria is met or the maximum number of trees are built. \n\n\n>Final_Prediction = (Overall_log_odds) + learning_rate * (tree_1_residual) + ... + learning_rate * (tree_n_residual)\n\n\n![xgb_diagram](files/shared_uploads/trevorj@berkeley.edu/xgb_diagram.png)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e164a145-911c-4efe-b678-d8b34cf20b38"}}},{"cell_type":"markdown","source":["## Ensemble Model\n\nThe ensemble model takes the binary classification predictions from the logistic regression, random forest, and xgboost models and combines them to make a final prediction for the classification. We used this to take the final predicted class from each model and vote on the final predicted class. Because we have three different models making a binary classification, there are never any tied votes. \n\nThe final predicted class in formula form can be thought of as follows:\n\n$$\n\\hat{y} = round(\\frac{\\hat{y}_{logistic} + \\hat{y}_{randomforest} + \\hat{y}_{xgboost}}{3})\n$$"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1fb66835-0ce5-484f-b27d-020b2acecd7c"}}},{"cell_type":"markdown","source":["# Experiments\n\nWe ran many experiments on different models for the flight prediciton problem. In this section, we provide details for each of our three models used in the ensemble."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83a365f9-10d4-4321-94d4-035b7c6f5c63"}}},{"cell_type":"markdown","source":["## Baseline Logistic Regression Model\n\nFor the baseline Logistic regression, here is the end-to-end modeling process:\nWe read check-pointed data after processing and feature engineering (8 seconds). To deal with the class imbalance of the problem, we `under-sampled` the majority class (i.e. flights _not_ delayed) in the training data to match the number of delayed flights, so we have a balanced training set (40 seconds). We dropped categorical and derived features that we weren't ready to process, leaving only 354 numerical features (4 seconds). We ran a standard mean/variance based scaling on the feature sets (1.9 minutes). \n\nWe performed a preliminary Principal Component Analysis and inspected the explained variance of the features (9.4 minutes). Based on an elbow of the individual explained variance of features, we decided to reduce the dimension of the feature space to 40. <br>\n  ![explained_variance](files/shared_uploads/yizhang7210@berkeley.edu/Screen_Shot_2022_04_09_at_11_15_30_PM.png) <br>\n- Performing the PCA again on the training data reducing to 40 features (9.5 minutes)\n- Training the Logistic Regression model (6.4 minutes)\n- Evaluate the model on the hold-out test set (i.e. flights in 2019) (3.1 minutes)\n\nWe did not perform any hyper-parameter tuning, and only used some reasonable default parameters for the Logistic Regression:\n- maxIter: 15\n- regParam (for regularizatoin penalty): 0.05\n- elasticNetParam (for mix between L2/L1 penalty): 0.05\n\nMore details on the baseline Logistic Regression Model can be found in [this notebook](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/1898361324231667/command/1898361324231668)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a2099058-1be8-42b5-9dbd-3a643a1cf3fa"}}},{"cell_type":"markdown","source":["## Refined Logistic Regression\n\nFor a more accurate model from the baseline, we made a number of improvements to the baseline Logistic Regression Model:\n\nWe made use of the categorical features in the Flights dataset and the derived features that the baseline model ignored. This was done via MLlib's built-in string indexer and one-hot encoder (25 seconds).\n\nFor PCA, we realized that identifying the target dimension using an elbow on the size of the individual explained variance was not ideal. In the baseline model, the first 40 dimensions ended up only actually explaining ~50% of total variance. <br>\n  ![cumulative_variance](files/shared_uploads/yizhang7210@berkeley.edu/Screen_Shot_2022_04_09_at_11_35_59_PM.png) <br>\n  \nAs a result, we increased the number of target dimensions to 125, which now explains ~90% of the variance in the data (3 minutes). We utlized the cross validation we set up earlier to perform hyper-parameter tuning via grid search. We were unable to fully automate the process or perform random searching because we were not able to successfully run very long Spark jobs. However, we were able to see the result of the combinations of the following hyper-parameters (approximately 4 hours total):\n  - regParam: 0.1, 0.2, 0.3\n  - elasticNetParam: 0.0, 0.3, 0.5\n  - maxIter: 15, 25\n  \nFurthermore, we also experimented with oversampling (with replacement) the minority class to make better use of the large amount of data that we had to capture more signals while still keeping the training set balanced. We tried ratios of 1.0, 1.5, and 2.0, and we did not see a visible difference in the resulting model performances (~3 hours total).\n\nOverall we found the best hyperparameter combination of the Logistic Regression model to be:\n  - maxIter: 25\n  - regParam: 0.1\n  - elasticNetParam: 0.0 (i.e. entirely L2)\n  - over-sample ratio: 2.0 (i.e. resample the minority classes twice in the training data, and add the same number of majority class examples)\n\nMore details on the refined Logistic Regression Model can be found in [this notebook](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/4070574709985129/command/1858507102396657)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b202f067-f167-48bb-8959-22cb1af2c568"}}},{"cell_type":"markdown","source":["## Random Forest & XGBoost Models\n\n\nFor the random forest classifier, we ran experiments to see which features were most important and which hyperparamters worked best. A plot of the most important features can be found in the decision tree models section, and we describe our hyperparameter tuning process in that section as well. We found that using a maximum tree depth of 12, 50 ensemble trees, and subsampling rate of 80% yielded strong performance through these experiments. \n\nFinally, we went through a similar process of experimentation with the XGBoost model. We explored the most important features and hyperparameters through experimentation. We found strong performance with each tree having a max depth of 10, 60 iterative trees with a learning rate of 0.1, 80% column sample by tree, gamma of .05, alpha of 0, and lambda of 0.1."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"63844e1a-d19e-4b94-be67-5b83707dda98"}}},{"cell_type":"markdown","source":["### Optimizing training time\n\nWhen fitting our final models, we wanted to train our final models using the entire full dataset. However, training time was very important to our process when hyperparameter tuning through cross validation. We had limited cluster resources and time to select optimal hyperparameters. Because of these we often took smaller samples of our training dataset, then performed cross validation on these smaller dataset portions in order to try out different hyperparameter selections. We generally chose 25% of the dataset to evaluate with for on set of cross validation testing."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f761de0d-5863-4cac-bdcb-4d7804eb9e21"}}},{"cell_type":"markdown","source":["## Ensemble Model\n\nFor our exciting and novel approach to modeling, we built an ensemble model of 3 different models to make a final prediction. The final ensemble model consisted of a logistic regression portion that used PCA, an XGBoost model, and Random Forest Classifier. Our ensemble model takes an input dataset and makes predictions using all three models. Then, it uses the 3 predicted classifications to take a vote in order to make the final classification.\n\nWe created several functions to complete the above steps for each model. We assembled these model functions into a custom class to easily make predictions on new data. A notebook containing this custom class with ensemble prediction methods is [found here](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/1858507102412201/command/1858507102412221). \n\nWe feel this ensemble approach will perform well on new unforseen data as it generalizes well. Training all three models took approximately 1.5 hours, and making predictions on the final 2019 data with this final ensemble pipeline took about 10 minutes."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7ba8fe9-b2af-4a26-afcb-6a525fe1b5ff"}}},{"cell_type":"markdown","source":["### Optimizing training time\n\nTraining time was extremely important given the deadlines we had and the time consuming cross validation. When performing hyperparameter tuning using 5-fold cross validation, we used a smaller dataset that was about 25% of the normal training dataset size. Ideally, we'd like to experiment on the full training data, however we took this approach to optimize training time. Then when fitting the final trained models, we chose to fit them on the full train dataset in order to maximize performance. \n\nFinally, when we fit our final ensemble model, we did so on the full training dataset. This process takes roughtly one and a half hours to fit all three models in the ensemble. Due to the time series nature of our dataset, we'll need to refit our model often. Thus keeping the training time optimized is important so we can do so frequently."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9394e9e1-c7b3-4298-8303-94a2162e046c"}}},{"cell_type":"markdown","source":["# Results and Discussion"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c866e96d-4a05-46ba-a872-399915028f59"}}},{"cell_type":"markdown","source":["## Baseline Logistic Regression\n\nThe baseline logistic regression model as described above yielded the following final performance on the full datasets:\n\nHere are the result metrics:\n- F-2 score: 0.613\n- Precision: 61.39%\n- Recall: 61.33%\n- Overall accuracy: 61.33%\n\nWith no hyper-parameter tuning, only numerical features, and a very basic logistic regression model, the overall performance of the baseline model looks respectable. We think under-sampling the majority class to have a balanced training set was a major contributor to having a fairly balanced performance here, with precision and recall both quite reasonable.\n\nThe reason why we did not use over-sampling the minority class was because one of the team members had prior experience comparing both over- and under-sampling, and under-sampling the majority class had performed better. This also had the added benefit of having a smaller overall training set, reducing the time taken to train the models."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"177a8824-cfc0-491f-8743-cfd3a2cb5fc4"}}},{"cell_type":"markdown","source":["## Refined Logistic Regression\n\nThe final evaluation of the refined logistic regression on the hold-out set including training and prediction took ~26 minutes, and here are the result metrics:\n- F-2 score: 0.702\n- Precision: 70.64%\n- Recall: 70.31%\n- Overall accuracy: 70.31%\n\nThis was a meaningful improvement to the baseline model. We think the hyper-parameter tuning played a major role here, with the `elasticNetParam` being a significant contributor. It seems L2 norm was a more appropriate metric here, hence having the penalty term to be entirely L2 made a difference. Another major contributor to the performance improvement was the derived features that were ignored in the baseline model. They seemed to have had good discriminative power."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a675527-f9f6-4bd7-a3a3-e9a08d4332c6"}}},{"cell_type":"markdown","source":["## XGBoost\n\nAfter performing the cross validation for tuning and selecting final hyperparameters described above, the final XGBoost classifier yielded the following performance on the full datasets: \n\n- Train F2 Score: 0.802\n- Test F2 Score: 0.804"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f170fc0-10a4-46b4-8de9-c7e8cce981eb"}}},{"cell_type":"markdown","source":["## Random Forest Classifier \n\nAfter performing the cross validation for tuning and selecting final hyperparameters described above, the final model yielded the following performance on the full datasets: \n\n- Train F2 Score: 0.792\n- Test F2 Score: 0.799"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b592b8ed-1026-4c18-9fab-3091a33b1e51"}}},{"cell_type":"markdown","source":["## Ensemble\n\nWhen we combined the predicted classes from all three models and used these to take a vote for the final prediction, we received the following performances: \n\n- Train F2 Score: 0.776\n- Test F2 Score: 0.714\n\nIndividually, the XGBoost model seems to outperform the rest in terms of F2 score. However, we chose to perform an ensemble model as our novel idea to test out. We also believe that this ensemble model could generalize better to future or unforeseen data because of the various techniques it uses. It does appear however that the model might be overfitting slightly because of the F2 score of train outperforming the F2 score on test. But, the difference between the two is small."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5987077e-efd1-4caa-b1a7-c42fd64a8003"}}},{"cell_type":"markdown","source":["# Algorithm Implementation\n\nWe implemented Logistic Regression from scratch, using PySpark's RDD interface directly.\n\nWe trained the model on a small slice of the Flights data (5665 data points), and tested it on 155 data points.\n\nOn this small scale example, the hand-crafted model had the following performance, around the same ball-park as the MLlib's implementation on the large data set:\n- accuracy: 0.626\n- precision: 0.377\n- recall: 0.745\n- F-1 score: 0.500\n- F-2 score: 0.622\n\nMore details on the implementation can be found in [this notebook](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/1731553363139939/command/1858507102373876)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"613e588b-895f-4489-a709-48c12acf6a8f"}}},{"cell_type":"markdown","source":["# Limitations, Challenges and Future Work"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4feced2d-12aa-43a3-b574-1b3ea2772049"}}},{"cell_type":"markdown","source":["## What Worked, What Didn't?\n\nHyperparameter tuning did not work as well as we would have liked. We were unable to perform the full hyperparameter tuning to the extent we wanted to given the time constraints and computing power. We often had to use smaller sampled versions of our dataset in order to actually see results in time. We also were only able to test out small ranges of potential hyperparameters. If given more time, we would have prefered to hyperparameter tune on a larger range of potential values, and do so on the full training dataset. \n\nUsing the original response variable balance did not work well when fitting models. We tried using the training dataset as-is to fit and predict models. But, we found that when rebalancing the data to a 50-50 response variable split through downsampling, the models performed much better. Thus this was beneficial for us to discover the increased performance of downsampling. \n\nFitting, saving, and loading transformers from the training dataset did not work as well as expected. When implementing on new unforseen data, the transformer objects often gave errors. We had to debug for a long time to figure out how to use these properly. However, in the end we were able to get our ensemble model to correctly make predictions on new unforseen data. \n\nUsing PCA in the logistic regression worked well. After one-hot-encoding the feature space and including the hundreds of weather measurements, the dataset was extremely sparse and included lots of potentially correlated features. PCA was able to condense the feature space into something much more manageable and barely lost any performance with the logistic regression model. Using PCA we retained 90% of the variance and made our model more parsimonious."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73134ec8-1221-4582-a2f3-41a858acfd58"}}},{"cell_type":"markdown","source":["## Performance\n\nThe training time of our models was highly dependent on cluster strength. As of April 10, our cluster had 6 workers that we could use to parallelize our tasks. Because we had many teams trying to fit and hyperparameter tune models over the past few weeks, the clusters may have been scaled back for some teams and full compute capacity was limited. This made it challenging for our team to hyperparameter tune over all possibilities we were interested in. We had to think more intuitively about the algorithm designs and think of the best hyperparameters that we would like to test given the time constraints. \n\nIn addition, each of our models in ensemble accounted for different characteristics from the data, so voting might not have been the best method to select the final classification."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca619761-b866-4f75-9eca-27b7719d702b"}}},{"cell_type":"markdown","source":["## Scalability\n\nBecause our data was time series based, our variables have seasonality and shifts over time. It is very important for us to keep our model trained with the latest data available. Thus, optimal training time is important for our problem. As-is, fitting all three models in our ensemble could take about an hour and a half to fit on the full training dataset. Perhaps as we collect new data, we'll want to refit our models and get rid of older data that won't be as indicative anymore. This training time of an hour and a half is not excessive, and our process should scale well as we refit with new data. Even if we had to refit our models daily, we could do so at the end of every day and the workload would be manageable."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2939d04-abe7-4c73-9d43-9aee5a5860e8"}}},{"cell_type":"markdown","source":["## Gap Analysis\n\nLooking at the leaderboard, there were not many teams that also used F-2 score as their evaluation metric. Out of those who did, our team has the highest F-2 score with our final ensemble model. However, we experimented with several different tweaks to try to improve this. One of these experiments involved tweaking the amount of undersampling done on the training data. We tried performing no undersampling and fitting on the full train dataset and compared that performance to the undersampled set. We found undersampling improved our F-2 score so we kept using this method. We also experimented with changing the elastic net parameter for the logistic regression model. For example, changing the elastic net parameter from 0.1 to 0.05 reduces the F-2 scores for the logistic regression from around 70% to around 61%. We also experimented with changing the feature set used in our decision tree models. This did not have a large impact on performance because the decision trees perform implicit variable selection. \n\nWhile we were not optimizing for F1 score when tuning our models, we did notice some groups have achieved higher F1 scores than our final ensemble model. The team with a higher F1 score than us is using a logistic regression + random forest ensemble classifier. This team is using deeper decision trees, and more iterations in their logistic regression than we are. We observed this after finalizing our model. Thus, it does give us some ideas of things we could have done to improve our model."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a445cbb-24ae-4c45-a26d-5d2559ba61d4"}}},{"cell_type":"markdown","source":["## Application of Course Concepts\n\nSeveral concepts that we learned in class were illustrated in our code for this project:\n\n- **One Hot Encoding**: One hot encoding is a pre-process to convert categorical variables into a form that could be provided to machine learning algorithms to improve prediction. We used logistic regression as our baseline model and it is unable to work with categorical variabls directly, so these categorical variables needed to be represented numerically in order to feed into our algorithm.\n\n- **Standarization of Variables**: Before features are fed into training models, they need to be standardized in order for machine learning models to converge. With this project, we have applied this concept to standardize the features for our baseline logistic regression model.\n\n- **Understanding transformations vs actions**: In class, we learned to distinguish between functions that are transformations and actions. Spark treats many functions as \"transformations\" which are lazily evaluated, and aren't actually performed on the full dataset until an action is triggered. It was important for us to keep this in mind when designing our code to think about how long a certain operation would take, and use proper caching to avoid re-running transformations when possible. Understanding this course concept improved developer time when we applied throughout our project. \n\n- **Algorithms Explored**: Throughout this class, we have learned about various of machine learning algorithms including supervised and unsupervised models. With this project we have explored Logistic Regression, Random Forest and Gradient Boosting Tree.\n\n- **Data Checkpointing**: When deal with large amount of data as in this project, the low cost of storage v.s. the high time cost of compute becomes apparent. For a number of processes, including joining datasets, performing PCA, even logistic regression training, it could take 30 minutes or more. However, if we perform the computation once and save the results to permanent storage, reading from it usually takes less than 1 minute. Only until the last few days of the project did we realize we could have more aggressively saved more data into storage, notablyl the prediction results from various models, to help with us putting the ensemble model together."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ec07155-073b-4b24-b89a-ba7482cbf9aa"}}},{"cell_type":"markdown","source":["## Future Work\n\nFor the logistic regression models, even though we achieved respectable F-2 scores, there are still a number of techniques we would like to explore, for example:\n- Use different probability threshold to make delayed/not-delayed prediction,\n- Experiment with more nuanced methods of deciding the final classification in our ensemble model rather than taking a vote,\n- More thorough hyper-parameter searching, including some form of random search, and\n- Try out having an imbalanced training set, in the hope to further increase recall and the F-2 score."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bdf01f3c-47d0-4e66-b724-a56440db26d4"}}},{"cell_type":"markdown","source":["# Conclusion\n \nIn this project, we built a flight delay prediction classifier system. In summary, we performed the following tasks:\n- Data processing and feature engineering, including adding derived time-based features\n- Modeling with algorithms such as Logistic Regression, Random Forests and Gradient Boosting Trees\n- Cross-validated models on a 5-fold time-rolling cross validation, and tested them on a hold-out dataset\n\nOverall, our best model performed fairly well and were able to reach a healthy balance of precision and recall, having had a peak F-2 score of 0.804, which we hope could help our stakeholders, i.e. airline executives, to devise strategies in real time to mitigate major flight delays.\n\nWe also took some learnings from this project:\n- Exploratory data analysis is very important. We derived many insights and ideas on what features and how to put various different data sets together through our EDA process\n- Data scale matters. With the need to process a large amount of data, every step requires more careful thought to make sure of its correctness, and every intermediate result could be check-pointed to save time\n- Ensemble model is not always better. Even with 3 fairly sophisticated and tuned models, the emsemble model still did not outperform some individual models."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"449b5898-42bd-4e25-8aa9-88048fdae438"}}},{"cell_type":"markdown","source":["# References\n\n[1] https://www.zippia.com/advice/airline-industry-statistics/\n\n[2] https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8903554&casa_token=Lobmd1rRmqwAAAAA:UQEoNuwul2BWBO3XOG4RLngJJtTfOHwc_X91bQcgsqyTxFTWhxa5vBiFj0WI95crqKsnoLc5ZUE&tag=1\n\n[3] https://iopscience.iop.org/article/10.1088/1755-1315/81/1/012198/pdf\n\n[4] https://www.sciencedirect.com/science/article/pii/S092523122101612X?casa_token=vcJPeqpJSxEAAAAA:Lks4HO1Xg2xfM7-rAMN1gj9kLaC1XBbA0X96IS7AjH8JCrYIrrNCD_Lcih9XKkrP5UO0SxtpnP0\n\n[5] https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/\n\n[6] https://deepai.org/machine-learning-glossary-and-terms/f-score\n\n[7] https://medium.com/@soumyachess1496/cross-validation-in-time-series-566ae4981ce4\n\n[8] https://www.faa.gov/newsroom/inclement-weather-0?newsId=23074#:~:text=Inclement%20weather%2C%20including%20thunderstorms%2C%20snowstorms,70%20percent%20of%20all%20delays\n\n[9] https://towardsdatascience.com/imputing-numerical-data-top-5-techniques-every-data-scientist-must-know-587c0f51552a"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1de81fc9-6225-44db-8831-05371e96f2d6"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"W261_SP22_FINAL_PROJECT_TEAM5","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1858507102380714}},"nbformat":4,"nbformat_minor":0}
