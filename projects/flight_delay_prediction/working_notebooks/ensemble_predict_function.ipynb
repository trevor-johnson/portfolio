{"cells":[{"cell_type":"code","source":["from pyspark.sql import types, Window, functions as F\nimport pandas as pd\nimport numpy as np\nfrom pyspark.ml.feature import OneHotEncoder, OneHotEncoderModel, StringIndexer, StringIndexerModel, StandardScaler, StandardScalerModel, PCA, PCAModel, MinMaxScaler, MinMaxScalerModel, VectorAssembler\nfrom pyspark.ml.classification import GBTClassifier, RandomForestClassifier, RandomForestClassificationModel, LogisticRegressionModel, LogisticRegression\nfrom pyspark.mllib.evaluation import MulticlassMetrics\nfrom sparkdl.xgboost import XgboostClassifier, XgboostClassifierModel"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e008678-877e-4d4a-8fd6-1c56c248280e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# The Class"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d028579c-eb45-488e-9d16-4c68b0c07452"}}},{"cell_type":"code","source":["class EnsemblePredict:\n    \n    def __init__(self):\n        pass\n        \n    def xgb_predict(self, df):\n        '''\n        Calculated predictions on the dataset you pass in. \n        Use function on all_time_full_join_6 without any data cleaning.\n        All data cleaning is done within this function.\n\n        Output:\n        spark.DataFrame object with two new columns: xgb_prob, and xgb_prediction\n        '''\n\n        # vars to use in the model\n        X_vars = [\n            # time vars\n            'YEAR_AIRLNS', 'QUARTER_AIRLNS', 'MONTH_AIRLNS', 'DAY_OF_WEEK_AIRLNS', \n\n            # airport location stuff\n            'CRS_ELAPSED_TIME_AIRLNS', 'DISTANCE_AIRLNS', 'ELEVATION_WTHR_origin', 'ELEVATION_WTHR_dest', \n            'LATITUDE_WTHR_origin', 'LONGITUDE_WTHR_origin', 'LATITUDE_WTHR_dest', 'LONGITUDE_WTHR_dest',\n\n            # airport cat vars to encode/index\n            'ORIGIN_AIRLNS', 'DEST_AIRLNS', 'OP_UNIQUE_CARRIER_AIRLNS', \n\n            # weather vars origin\n            'WND_WTHR_direction_angle_origin', 'WND_WTHR_speed_rate_origin', 'TMP_WTHR_air_temperature_origin', 'DEW_WTHR_dew_point_temperature_origin',\n            'VIS_WTHR_distance_dimension_origin', 'GA1_WTHR_base_height_dimension_origin', 'GF1_WTHR_lowest_cloud_base_height_dimension_origin', \n            'AA1_WTHR_period_quantity_in_hours_origin', 'AA1_WTHR_depth_dimension_origin', \n\n            # same weather vars, but for dest\n            'WND_WTHR_direction_angle_dest', 'WND_WTHR_speed_rate_dest', 'TMP_WTHR_air_temperature_dest', 'DEW_WTHR_dew_point_temperature_dest',\n            'VIS_WTHR_distance_dimension_dest', 'GA1_WTHR_base_height_dimension_dest', 'GF1_WTHR_lowest_cloud_base_height_dimension_dest', \n            'AA1_WTHR_period_quantity_in_hours_dest', 'AA1_WTHR_depth_dimension_dest', \n\n            # esther feature eng\n            'LOCAL_DEP_HOUR', 'HOLIDAY', 'Prev_Flight_Delay_15', 'Enough_Time_Btwn_Estimate_Arrival_and_Planned_Dep', 'Poor_Schedule'\n        ]\n\n        y_var = 'DEP_DEL15_AIRLNS'\n\n        # create an id column for final join\n        df = df.withColumn(\"xgb_id\", F.monotonically_increasing_id())\n\n        # impute some missing values\n        df2 = df.na.fill(0)\n#         df2 = df.na.fill({\n#             'LATITUDE_WTHR_origin': 0\n#             ,'LONGITUDE_WTHR_origin': 0\n#             ,'ELEVATION_WTHR_origin': 0\n#             ,'LATITUDE_WTHR_dest': 0\n#             ,'LONGITUDE_WTHR_dest': 0\n#             ,'ELEVATION_WTHR_dest': 0\n#             ,'Prev_Flight_Delay_15': 0\n#             #,'DEP_DEL15_AIRLNS': 0\n#         })\n\n        # cast some vars to int\n        str_cols = ['Prev_Flight_Delay_15', 'Poor_Schedule', 'Enough_Time_Btwn_Estimate_Arrival_and_Planned_Dep']\n        for column in str_cols:\n            df2 = df2.withColumn(column, F.col(column).cast(types.IntegerType())) \n\n        # vars to index\n        # Specify which columns to index (ie cast to int)\n        vars_to_index = [\n            'ORIGIN_AIRLNS', \n            'DEST_AIRLNS', \n            'OP_UNIQUE_CARRIER_AIRLNS' # a more granular form of origin/dest airlines\n        ]\n\n        # rename cols to drop them later\n        for var in vars_to_index:\n            df2 = df2.withColumnRenamed(var, var+'_old')\n\n        # finally, index them\n        xgb_indexer = StringIndexerModel.load('dbfs:/' + 'files/shared_uploads/trevorj@berkeley.edu/xgb_indexer_1')\n        xgb_indexer = xgb_indexer.setHandleInvalid('keep')\n        df2 = xgb_indexer.transform(df2)\n        #indexer = StringIndexer(inputCols=[i+'_old' for i in vars_to_index], outputCols=vars_to_index)\n        #df2 = indexer.fit(df2).transform(df2)\n        df2 = df2.drop(*[i+'_old' for i in vars_to_index])\n\n        # vectorize\n        df2 = df2.select(X_vars + [y_var])\n        vectorAssembler = VectorAssembler(inputCols = X_vars, outputCol = 'features', handleInvalid='skip')\n        df2 = vectorAssembler.transform(df2).select(['features', y_var])\n\n        # make predictions\n        # load the final fitted xgb model\n        # model_path = 'files/shared_uploads/trevorj@berkeley.edu/xgb_0408_v1' # 0.5692620\n        model_path = 'files/shared_uploads/trevorj@berkeley.edu/xgb_0408_v2' # \n        xgb_fit = XgboostClassifierModel.load('dbfs:/' + model_path)\n        df2 = xgb_fit.transform(df2)\n\n        # Extract probabilities\n        get_item=F.udf(lambda v:float(v[1]), types.FloatType())\n        df2 = df2.withColumn(\"xgb_prob\", get_item('probability'))\n        df2 = df2.withColumnRenamed('prediction', 'xgb_prediction')\n        df2 = df2.select('xgb_prob', 'xgb_prediction')\n        df2 = df2.withColumn('xgb_id', F.monotonically_increasing_id())\n\n        # join preds to original dataset and return it\n        df = df = df.join(df2, on='xgb_id', how='left').drop('xgb_id')\n        \n        return df\n    \n    \n    def rf_predict(self, df):\n        '''\n        Calculated predictions on the dataset you pass in. \n        Use function on all_time_full_join_6 without any data cleaning.\n        All data cleaning is done within this function.\n\n        Output:\n        spark.DataFrame object with two new columns: rf_prob, and rf_prediction\n        '''\n\n        # load the final fitted model\n        model_path = 'files/shared_uploads/trevorj@berkeley.edu/rf_0409_v2'\n        rf_fit = RandomForestClassificationModel.load('dbfs:/' + model_path)\n\n        # vars to use in the model\n        X_vars = [\n        # time vars\n        'YEAR_AIRLNS', 'QUARTER_AIRLNS', 'MONTH_AIRLNS', 'DAY_OF_WEEK_AIRLNS', 'CRS_DEP_TIME_AIRLNS', 'CRS_ARR_TIME_AIRLNS', \n\n        # airport location stuff\n        'CRS_ELAPSED_TIME_AIRLNS', 'DISTANCE_AIRLNS', 'ELEVATION_WTHR_origin', 'ELEVATION_WTHR_dest', \n        'LATITUDE_WTHR_origin', 'LONGITUDE_WTHR_origin', 'LATITUDE_WTHR_dest', 'LONGITUDE_WTHR_dest',\n\n        # airport cat vars to encode/index\n        'ORIGIN_AIRLNS', 'DEST_AIRLNS', 'OP_UNIQUE_CARRIER_AIRLNS', \n\n        # weather vars origin\n        'WND_WTHR_direction_angle_origin', 'WND_WTHR_speed_rate_origin', 'TMP_WTHR_air_temperature_origin', 'DEW_WTHR_dew_point_temperature_origin',\n        'VIS_WTHR_distance_dimension_origin', 'GA1_WTHR_base_height_dimension_origin', 'GF1_WTHR_lowest_cloud_base_height_dimension_origin', \n        'AA1_WTHR_period_quantity_in_hours_origin', 'AA1_WTHR_depth_dimension_origin', 'AA2_WTHR_depth_dimension_origin', \n        'AJ1_WTHR_equivalent_water_depth_dimension_origin', 'AN1_WTHR_depth_dimension_origin', \n        'AL1_WTHR_period_quantity_origin', 'AL1_WTHR_depth_dimension_origin', 'SLP_WTHR_sea_level_pressure_origin',\n        'GA1_WTHR_coverage_code_origin-00', 'GF1_WTHR_total_coverage_code_origin-00', 'AA1_WTHR_condition_code_origin-3', 'AU1_WTHR_descriptor_code_origin-0',\n        'AU1_WTHR_descriptor_code_origin-7', 'AU1_WTHR_obscuration_code_origin-0', 'AU1_WTHR_other_weather_phenomena_code_origin-0', \n\n        # same weather vars, but for dest\n        'WND_WTHR_direction_angle_dest', 'WND_WTHR_speed_rate_dest', 'TMP_WTHR_air_temperature_dest', 'DEW_WTHR_dew_point_temperature_dest',\n        'VIS_WTHR_distance_dimension_dest', 'GA1_WTHR_base_height_dimension_dest', 'GF1_WTHR_lowest_cloud_base_height_dimension_dest', \n        'AA1_WTHR_period_quantity_in_hours_dest', 'AA1_WTHR_depth_dimension_dest', 'AA2_WTHR_depth_dimension_dest', \n        'AJ1_WTHR_equivalent_water_depth_dimension_dest', 'AN1_WTHR_depth_dimension_dest', \n        'AL1_WTHR_period_quantity_dest', 'AL1_WTHR_depth_dimension_dest', 'SLP_WTHR_sea_level_pressure_dest', \n        'GA1_WTHR_coverage_code_dest-00', 'GF1_WTHR_total_coverage_code_dest-00', 'AA1_WTHR_condition_code_dest-3',\n        'AU1_WTHR_descriptor_code_dest-7', 'AU1_WTHR_obscuration_code_dest-0', 'AU1_WTHR_other_weather_phenomena_code_dest-0', \n\n        # esther feature eng\n        'LOCAL_DEP_HOUR', 'HOLIDAY', 'Prev_Flight_Delay_15', 'Enough_Time_Btwn_Estimate_Arrival_and_Planned_Dep', 'Poor_Schedule'\n        ]\n        features=X_vars\n\n        y_var = 'DEP_DEL15_AIRLNS'\n\n        # create an id column for final join\n        df = df.withColumn(\"rf_id\", F.monotonically_increasing_id())\n\n        df2 = df.alias('df2')\n\n        # cast to int\n        str_cols = ['Prev_Flight_Delay_15', 'Poor_Schedule', 'Enough_Time_Btwn_Estimate_Arrival_and_Planned_Dep']\n        for column in str_cols:\n            df2 = df2.withColumn(column, F.col(column).cast(types.IntegerType())) \n\n        # impute some missing values\n        df2 = df2.na.fill(0)\n\n        # get fields\n        features = [i for i in df2.columns if i != \"DEP_DEL15_AIRLNS\"]\n        str_cols = [t[0] for t in df2.dtypes if t[1] == 'string' and t[0] in features]\n        # drop some features\n        #features.remove('FL_DATE_AIRLNS')\n\n        # index all str columns    \n        vars_to_index = [i for i in str_cols if i != 'FL_DATE_AIRLNS']\n\n        # rename cols to drop them later\n        for var in vars_to_index:\n            df2 = df2.withColumnRenamed(var, var+'_old')\n\n        # finally, index them\n        rf_indexer = StringIndexerModel.load('dbfs:/' + 'files/shared_uploads/trevorj@berkeley.edu/rf_indexer_2')\n        rf_indexer = rf_indexer.setHandleInvalid('keep')\n        df2 = rf_indexer.transform(df2)\n        df2 = df2.drop(*[i+'_old' for i in vars_to_index])\n\n        # vectorize\n        df2 = df2.select(X_vars + [y_var])\n        vectorAssembler = VectorAssembler(inputCols = X_vars, outputCol = 'features', handleInvalid='skip')\n        df2 = vectorAssembler.transform(df2).select(['features', y_var])\n\n        # make predictions\n        df2 = rf_fit.transform(df2)\n\n        # Extract probabilities\n        get_item=F.udf(lambda v:float(v[1]), types.FloatType())\n        df2 = df2.withColumn(\"rf_prob\", get_item('probability'))\n        df2 = df2.withColumnRenamed('prediction', 'rf_prediction')\n        df2 = df2.select('rf_prob', 'rf_prediction')\n        df2 = df2.withColumn('rf_id', F.monotonically_increasing_id())\n\n        # join preds to original dataset and return it\n        df = df = df.join(df2, on='rf_id', how='left').drop('rf_id')\n        \n        return df\n    \n    \n    def lr_predict(self, df):\n        \n        df_original = df.alias('df_original')\n\n        categorical_string_features = [\n          'ORIGIN_AIRLNS',\n          'DEST_AIRLNS',\n          'OP_UNIQUE_CARRIER_AIRLNS'\n        ]\n\n        categorical_features = [\n          'ORIGIN_AIRLNS_indexed',\n          'DEST_AIRLNS_indexed',\n          'OP_UNIQUE_CARRIER_AIRLNS_indexed',\n          'HOLIDAY',\n          'Prev_Flight_Delay_15',\n          'Enough_Time_Btwn_Estimate_Arrival_and_Planned_Dep'\n        ]\n        \n        # run thru the full process on the full df_train\n        blob_container = \"main-storage\" # The name of your container created in https://portal.azure.com\n        storage_account = \"team05w261\" # The name of your Storage account created in https://portal.azure.com\n        secret_scope = \"team05\" # The name of the scope created in your local computer using the Databricks CLI\n        secret_key = \"team05-key\" # The name of the secret key created in your local computer using the Databricks CLI \n        blob_url = f\"wasbs://{blob_container}@{storage_account}.blob.core.windows.net\"\n        mount_path = \"/mnt/mids-w261\"\n\n        # Configure blob storage account access key globally\n        spark.conf.set(\n          f\"fs.azure.account.key.{storage_account}.blob.core.windows.net\",\n          dbutils.secrets.get(scope = secret_scope, key = secret_key)\n        )\n\n        df_train = spark.read.parquet(f\"{blob_url}/all_time_full_join_6\").filter(F.col('YEAR_AIRLNS') <= 2018).filter(F.col('DEP_DEL15_AIRLNS').isNotNull())\n        df_full = spark.read.parquet(f\"{blob_url}/all_time_full_join_6\")\n        # drop other nas\n        #df_train = df_train.dropna()\n        # try imputing NAs\n        df_full = df_full.na.fill(0)\n        df_train = df_train.na.fill(0)\n        # index\n        \n        \n\n        # returning error about unseen labels. So set this to skip\n        # Unseen label: PSE. To handle unseen labels, set Param handleInvalid to keep\n        indexer = StringIndexer(inputCols=categorical_string_features, outputCols=[col + '_indexed' for col in categorical_string_features], handleInvalid='skip')\n        #index_model = indexer.fit(df_train)\n        index_model = indexer.fit(df_full) # try fitting on full data to see if this works better\n        df_indexed = index_model.transform(df_train).drop(*categorical_string_features)\n        df_full = index_model.transform(df_full).drop(*categorical_string_features)\n        # encode\n        df_indexed = df_indexed.na.fill(value=0,subset=[\"Prev_Flight_Delay_15\"])\n        encoder = OneHotEncoder(inputCols=categorical_features, outputCols=[col + '_vec' for col in categorical_features])\n        #encoder_model = encoder.fit(df_indexed)\n        encoder_model = encoder.fit(df_full)\n        df_encoded = encoder_model.transform(df_indexed).drop(*categorical_features)\n        df_full = encoder_model.transform(df_full).drop(*categorical_features)\n        # Start putting data into trainable form\n        actual_feature_columns = [i for i in df_encoded.columns if i not in ['DEP_DEL15_AIRLNS', 'FL_DATE_AIRLNS']]\n        #actual_feature_columns.remove('DEP_DEL15_AIRLNS')\n        #actual_feature_columns.remove('FL_DATE_AIRLNS')\n        vectorAssembler = VectorAssembler(inputCols = actual_feature_columns, outputCol = 'features', handleInvalid='skip')\n        df_ready = vectorAssembler.transform(df_encoded).select(['features', 'DEP_DEL15_AIRLNS', 'FL_DATE_AIRLNS']).withColumnRenamed(\"DEP_DEL15_AIRLNS\", \"label\")\n        df_full = vectorAssembler.transform(df_full).select(['features', 'DEP_DEL15_AIRLNS', 'FL_DATE_AIRLNS']).withColumnRenamed(\"DEP_DEL15_AIRLNS\", \"label\")\n        # undersample\n        def undersample(data, label_col='DEP_DEL15_AIRLNS'):\n            delayed = data.filter(F.col(label_col) > 0)\n            not_delayed = data.filter(F.col(label_col) == 0)\n            delayed_count = delayed.count()\n            not_delayed_count = not_delayed.count()\n            sample_fraction = delayed_count * 1.0 / not_delayed_count\n            sample_not_delayed = not_delayed.sample(fraction=sample_fraction, seed=1)\n            return sample_not_delayed.union(delayed)\n        \n        df_train = undersample(df_ready)\n        df_train_ready = df_train.drop('FL_DATE_AIRLNS')\n        df_full = df_full.drop('FL_DATE_AIRLNS')\n        # min max scale\n        minMaxScaler = MinMaxScaler(inputCol='features', outputCol='features_scaled')\n        #scaler = minMaxScaler.fit(df_train_ready)\n        scaler = minMaxScaler.fit(df_full)\n        df_train_ready = scaler.transform(df_train_ready).select('features_scaled', 'label')\n        # PCA\n        pca = PCA(k=125, inputCol='features_scaled', outputCol='features_transformed')\n        pca_model = pca.fit(df_train_ready)\n        df_train_ready = pca_model.transform(df_train_ready)\n        # Train LR\n        df_train_ready = df_train_ready.select(['features_transformed', 'label']).withColumnRenamed(\"features_transformed\", \"features\")\n        lr = LogisticRegression(maxIter=25, regParam=.1, elasticNetParam=0)\n        lrModel = lr.fit(df_train_ready)\n        \n        \n        \n        # now run those fitted pieces on the input data\n        # index\n        df = index_model.transform(df).drop(*categorical_string_features)\n        # encode\n        df = df.na.fill(value=0,subset=[\"Prev_Flight_Delay_15\"])\n        df = df.na.fill(value=0) # lots of missing values, impute them\n        df = encoder_model.transform(df).drop(*categorical_features)\n        # vectorize\n        df = vectorAssembler.transform(df).select(['features', 'DEP_DEL15_AIRLNS', 'FL_DATE_AIRLNS']).withColumnRenamed(\"DEP_DEL15_AIRLNS\", \"label\")\n        # scale\n        df = scaler.transform(df).select('features_scaled', 'label')\n        # PCA\n        df = pca_model.transform(df)\n        # LR\n        df = lrModel.transform(df.withColumnRenamed(\"features_transformed\", \"features\"))\n        # Extract probabilities\n        get_item=F.udf(lambda v:float(v[1]), types.FloatType())\n        df = df.withColumn(\"lr_prob\", get_item('probability'))\n        df = df.withColumnRenamed('prediction', 'lr_prediction')\n        df = df.select('lr_prob', 'lr_prediction')\n        df = df.withColumn('lr_id', F.monotonically_increasing_id())\n\n        # join preds to original dataset and return it\n        df_original = df_original.withColumn('lr_id', F.monotonically_increasing_id()).join(df, on='lr_id', how='left').drop('lr_id')\n\n        return df_original\n    \n    \n    def ensemble_predict(self, df, verbose=False):\n        \n        if verbose:\n            print('Making random forest predictions')\n        df_rf = self.rf_predict(df).withColumn('id', F.monotonically_increasing_id())\n        \n        if verbose:\n            print('Making XGBoost predictions')\n        df_xgb = self.xgb_predict(df).select('xgb_prob', 'xgb_prediction').withColumn('id', F.monotonically_increasing_id())\n        \n        if verbose:\n            print('Making Logistic Regression predictions')\n        df_lr = self.lr_predict(df).select('lr_prob', 'lr_prediction').withColumn('id', F.monotonically_increasing_id())\n        \n        # join them\n        df_all = df_rf.join(df_xgb, on='id', how='left').join(df_lr, on='id', how='left')\n        \n        # sometimes the indexer needs to skip a few unforseen airlines. In this case let's just predict 0 for now. Can possibly revisit this later. \n        df_all = df_all.na.fill({\n            'lr_prediction': 0, 'rf_prediction': 0, 'xgb_prediction': 0,\n            'lr_prob': .49, 'rf_prob': .49, 'xgb_prob': .49})\n        \n        # final predicted class\n        df_all = df_all.withColumn('final_prediction', F.when((F.col('rf_prediction') + F.col('xgb_prediction') + F.col('lr_prediction')) >= 2, 1).otherwise(0))\n        df_all = df_all.withColumn('final_prob', (F.col('rf_prob') + F.col('xgb_prob') + F.col('lr_prob')) / 3)\n        \n        return df_all\n        "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"59345954-21b4-4b35-8035-8eb6443404d5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Functions for classification metrics"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22fa1ede-39ba-4a90-8e82-040628e45ee2"}}},{"cell_type":"code","source":["# return f1 and f2 scores\ndef eval_class_metrics(df_results_train, df_results_test):\n\n    metrics_train = MulticlassMetrics(df_results_train.select('prediction', 'label').rdd)\n    metrics_test = MulticlassMetrics(df_results_test.select('prediction', 'label').rdd)\n    \n    print('\\nTrain F1 score from package')\n    print(metrics_train.weightedFMeasure(1.0))\n    print('Test F1 score from package')\n    print(metrics_test.weightedFMeasure(1.0))\n\n    print('\\nTrain F2 score from package')\n    print(metrics_train.weightedFMeasure(2.0))\n    print('Test F2 score from package')\n    print(metrics_test.weightedFMeasure(2.0))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e596e8bc-94fb-45c5-9764-952a367ba9eb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["blob_container = \"main-storage\" # The name of your container created in https://portal.azure.com\nstorage_account = \"team05w261\" # The name of your Storage account created in https://portal.azure.com\nsecret_scope = \"team05\" # The name of the scope created in your local computer using the Databricks CLI\nsecret_key = \"team05-key\" # The name of the secret key created in your local computer using the Databricks CLI \nblob_url = f\"wasbs://{blob_container}@{storage_account}.blob.core.windows.net\"\nmount_path = \"/mnt/mids-w261\"\n\n# Configure blob storage account access key globally\nspark.conf.set(\n  f\"fs.azure.account.key.{storage_account}.blob.core.windows.net\",\n  dbutils.secrets.get(scope = secret_scope, key = secret_key)\n)\n\ndf = spark.read.parquet(f\"{blob_url}/all_time_full_join_6\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e98348d0-87d8-4bd2-8458-d445b8f68c22"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Model Evaluation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"928a58d1-037d-4041-9c57-cafb3e6d6e53"}}},{"cell_type":"markdown","source":["## XGBoost"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56b81452-8617-4b68-8ec5-f9b108adc3c7"}}},{"cell_type":"code","source":["model = EnsemblePredict()\ndf = spark.read.parquet(f\"{blob_url}/all_time_full_join_6\")\ndf_train = model.xgb_predict(df.filter(F.col('YEAR_AIRLNS') <= 2018).filter(F.col('DEP_DEL15_AIRLNS').isNotNull()))\ndf_test = model.xgb_predict(df.filter(F.col('YEAR_AIRLNS') == 2019).filter(F.col('DEP_DEL15_AIRLNS').isNotNull()))\n\ndf_results_train = df_train.select('xgb_prediction', 'xgb_prob', 'DEP_DEL15_AIRLNS')\\\n    .withColumnRenamed('xgb_prediction', 'prediction')\\\n    .withColumnRenamed('DEP_DEL15_AIRLNS', 'label')\\\n    .withColumnRenamed('xgb_prob', 'probability')\n\ndf_results_test = df_test.select('xgb_prediction', 'xgb_prob', 'DEP_DEL15_AIRLNS')\\\n    .withColumnRenamed('xgb_prediction', 'prediction')\\\n    .withColumnRenamed('DEP_DEL15_AIRLNS', 'label')\\\n    .withColumnRenamed('xgb_prob', 'probability')\n\n\neval_class_metrics(df_results_train, df_results_test)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2afc0291-143e-4387-9f54-c59bfabffa05"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">\nTrain F1 score from package\n0.8110713455333177\nTest F1 score from package\n0.8112255196599087\n\nTrain F2 score from package\n0.8022678979896029\nTest F2 score from package\n0.804113509353127\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\nTrain F1 score from package\n0.8110713455333177\nTest F1 score from package\n0.8112255196599087\n\nTrain F2 score from package\n0.8022678979896029\nTest F2 score from package\n0.804113509353127\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Random Forest"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f57aa98-3a78-4256-8953-2b8a4262ce9a"}}},{"cell_type":"code","source":["model = EnsemblePredict()\ndf = spark.read.parquet(f\"{blob_url}/all_time_full_join_6\")\ndf_train = model.rf_predict(df.filter(F.col('YEAR_AIRLNS') <= 2018).filter(F.col('DEP_DEL15_AIRLNS').isNotNull()))\ndf_test = model.rf_predict(df.filter(F.col('YEAR_AIRLNS') == 2019).filter(F.col('DEP_DEL15_AIRLNS').isNotNull()))\n\ndf_results_train = df_train.select('rf_prediction', 'rf_prob', 'DEP_DEL15_AIRLNS')\\\n    .withColumnRenamed('rf_prediction', 'prediction')\\\n    .withColumnRenamed('DEP_DEL15_AIRLNS', 'label')\\\n    .withColumnRenamed('rf_prob', 'probability')\n\ndf_results_test = df_test.select('rf_prediction', 'rf_prob', 'DEP_DEL15_AIRLNS')\\\n    .withColumnRenamed('rf_prediction', 'prediction')\\\n    .withColumnRenamed('DEP_DEL15_AIRLNS', 'label')\\\n    .withColumnRenamed('rf_prob', 'probability')\n\neval_class_metrics(df_results_train, df_results_test)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00a85c2e-90f9-4d24-a9ff-171b0595ebe5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">\nTrain F1 score from package\n0.8014259954504102\nTest F1 score from package\n0.8059284906985623\n\nTrain F2 score from package\n0.7923217699233\nTest F2 score from package\n0.798850133421965\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\nTrain F1 score from package\n0.8014259954504102\nTest F1 score from package\n0.8059284906985623\n\nTrain F2 score from package\n0.7923217699233\nTest F2 score from package\n0.798850133421965\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Logistic Regression"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df5e2d8a-6302-4d76-874b-5cc803f52e74"}}},{"cell_type":"code","source":["# ran it once, and saved results to blob\n# model = EnsemblePredict()\n# df = spark.read.parquet(f\"{blob_url}/all_time_full_join_6\")\n# df_train = model.rf_predict(df.filter(F.col('YEAR_AIRLNS') <= 2018).filter(F.col('DEP_DEL15_AIRLNS').isNotNull()))\n# df_test = model.rf_predict(df.filter(F.col('YEAR_AIRLNS') == 2019).filter(F.col('DEP_DEL15_AIRLNS').isNotNull()))\n\n# df_train.write.parquet(f\"{blob_url}/lr_predictions_train_tj\")\n# df_test.write.parquet(f\"{blob_url}/lr_predictions_test_tj\")\n\n# from yi\n# df_train = spar.read.parquet(f\"{blob_url}/final_lr_train_predictions_1\")\n# df_test = spark.read.parquet(f\"{blob_url}/final_lr_test_predictions_1\")\n\ndf_train = spark.read.parquet(f\"{blob_url}/lr_predictions_train_tj\")\ndf_test = spark.read.parquet(f\"{blob_url}/lr_predictions_test_tj\")\n\n\ndf_results_train = df_train.select('lr_prediction', 'lr_prob', 'DEP_DEL15_AIRLNS')\\\n    .withColumnRenamed('lr_prediction', 'prediction')\\\n    .withColumnRenamed('DEP_DEL15_AIRLNS', 'label')\\\n    .withColumnRenamed('lr_prob', 'probability')\n\ndf_results_test = df_test.select('lr_prediction', 'lr_prob', 'DEP_DEL15_AIRLNS')\\\n    .withColumnRenamed('lr_prediction', 'prediction')\\\n    .withColumnRenamed('DEP_DEL15_AIRLNS', 'label')\\\n    .withColumnRenamed('lr_prob', 'probability')\n\n\neval_class_metrics(df_results_train, df_results_test)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28c3c489-2e53-4e10-9da3-72b945eb90aa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">\nTrain F1 score from package\n0.7669342090258459\nTest F1 score from package\n0.7718615348343423\n\nTrain F2 score from package\n0.7503474013711022\nTest F2 score from package\n0.7574821297229931\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\nTrain F1 score from package\n0.7669342090258459\nTest F1 score from package\n0.7718615348343423\n\nTrain F2 score from package\n0.7503474013711022\nTest F2 score from package\n0.7574821297229931\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Ensemble"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"18a5be85-79dd-4124-8dac-da1cf03b529d"}}},{"cell_type":"code","source":["# ensemble\n# model = EnsemblePredict()\n# df_train = model.ensemble_predict(df.filter(F.col('YEAR_AIRLNS') <= 2018))\n# df_test = model.ensemble_predict(df.filter(F.col('YEAR_AIRLNS') == 2019))\n\n# write to disc\n# df_train.write.parquet(f\"{blob_url}/ensemble_predictions_train\")\n# df_test.write.parquet(f\"{blob_url}/ensemble_predictions_test\")\n\n# start from here\ndf_train = spark.read.parquet(f\"{blob_url}/ensemble_predictions_train\").filter(F.col('DEP_DEL15_AIRLNS').isNotNull())\ndf_test = spark.read.parquet(f\"{blob_url}/ensemble_predictions_test\").filter(F.col('DEP_DEL15_AIRLNS').isNotNull())\n\ndf_results_train = df_train.select('final_prediction', 'final_prob', 'DEP_DEL15_AIRLNS')\\\n    .withColumnRenamed('final_prediction', 'prediction').withColumn('prediction', F.col('prediction').cast(types.DoubleType()))\\\n    .withColumnRenamed('DEP_DEL15_AIRLNS', 'label').withColumn('label', F.col('label').cast(types.DoubleType()))\\\n    .withColumnRenamed('final_prob', 'probability')\n\n\ndf_results_test = df_test.select('final_prediction', 'final_prob', 'DEP_DEL15_AIRLNS')\\\n    .withColumnRenamed('final_prediction', 'prediction').withColumn('prediction', F.col('prediction').cast(types.DoubleType()))\\\n    .withColumnRenamed('DEP_DEL15_AIRLNS', 'label').withColumn('label', F.col('label').cast(types.DoubleType()))\\\n    .withColumnRenamed('final_prob', 'probability')\n\neval_class_metrics(df_results_train, df_results_test)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"834b2a45-ee6d-46c3-b55e-678fbb975e23"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">\nTrain F1 score from package\n0.738426661250065\nTest F1 score from package\n0.7130083006364278\n\nTrain F2 score from package\n0.7757094432155528\nTest F2 score from package\n0.7141145509809816\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\nTrain F1 score from package\n0.738426661250065\nTest F1 score from package\n0.7130083006364278\n\nTrain F2 score from package\n0.7757094432155528\nTest F2 score from package\n0.7141145509809816\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"daaf0b72-7de0-45fd-a0db-f365c0609dd7"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ensemble_predict_function_v2","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1858507102412201}},"nbformat":4,"nbformat_minor":0}
