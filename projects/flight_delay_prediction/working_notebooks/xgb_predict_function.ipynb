{"cells":[{"cell_type":"code","source":["# Run this script to load in the master xgb_predict() function. \n# This function should be run on the all_time_full_join_6 without any data alterations. \n# Note the model saved at this path as the following parameters: 'files/shared_uploads/trevorj@berkeley.edu/xgb_0408_v1'\n## max_depth=7, \n## n_estimators=30, \n## learning_rate=.05, \n## colsample_bytree=.8, \n## gamma = .1, \n## reg_alpha = 0, \n## reg_lambda = 0\n\n## but the model saved at this path has the following hyperparams: 'files/shared_uploads/trevorj@berkeley.edu/xgb_0408_v2'\n# xgb_final = XgboostClassifier(labelCol=y_var, missing=0.0, max_depth=10, n_estimators=60, learning_rate=.1, colsample_bytree=.8, \n#                               gamma = .05, reg_alpha = 0, reg_lambda = .1).fit(df_train)\n## And the model is trained on a downsampled version of <=2018 data for an even 50-50 response variable split"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f4c721e-6748-4e53-8513-fde1895d6e89"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import types, Window, functions as F\nimport pandas as pd\nimport numpy as np\nfrom pyspark.ml.feature import VectorAssembler, StringIndexer\nfrom sparkdl.xgboost import XgboostClassifier, XgboostClassifierModel\nfrom pyspark.mllib.evaluation import MulticlassMetrics, BinaryClassificationMetrics"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f7f566b-4120-4d60-ad56-b62a20740b78"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def xgb_predict(df):\n    '''\n    Calculated predictions on the dataset you pass in. \n    Use function on all_time_full_join_6 without any data cleaning.\n    All data cleaning is done within this function.\n    \n    Output:\n    spark.DataFrame object with two new columns: xgb_prob, and xgb_prediction\n    '''\n    \n    # load the final fitted xgb model\n    # model_path = 'files/shared_uploads/trevorj@berkeley.edu/xgb_0408_v1' # 0.5692620\n    model_path = 'files/shared_uploads/trevorj@berkeley.edu/xgb_0408_v2' # \n    xgb_fit = XgboostClassifierModel.load('dbfs:/' + model_path)\n    \n    # vars to use in the model\n    X_vars = [\n        # time vars\n        'YEAR_AIRLNS', 'QUARTER_AIRLNS', 'MONTH_AIRLNS', 'DAY_OF_WEEK_AIRLNS', \n\n        # airport location stuff\n        'CRS_ELAPSED_TIME_AIRLNS', 'DISTANCE_AIRLNS', 'ELEVATION_WTHR_origin', 'ELEVATION_WTHR_dest', \n        'LATITUDE_WTHR_origin', 'LONGITUDE_WTHR_origin', 'LATITUDE_WTHR_dest', 'LONGITUDE_WTHR_dest',\n\n        # airport cat vars to encode/index\n        'ORIGIN_AIRLNS', 'DEST_AIRLNS', 'OP_UNIQUE_CARRIER_AIRLNS', \n\n        # weather vars origin\n        'WND_WTHR_direction_angle_origin', 'WND_WTHR_speed_rate_origin', 'TMP_WTHR_air_temperature_origin', 'DEW_WTHR_dew_point_temperature_origin',\n        'VIS_WTHR_distance_dimension_origin', 'GA1_WTHR_base_height_dimension_origin', 'GF1_WTHR_lowest_cloud_base_height_dimension_origin', \n        'AA1_WTHR_period_quantity_in_hours_origin', 'AA1_WTHR_depth_dimension_origin', \n\n        # same weather vars, but for dest\n        'WND_WTHR_direction_angle_dest', 'WND_WTHR_speed_rate_dest', 'TMP_WTHR_air_temperature_dest', 'DEW_WTHR_dew_point_temperature_dest',\n        'VIS_WTHR_distance_dimension_dest', 'GA1_WTHR_base_height_dimension_dest', 'GF1_WTHR_lowest_cloud_base_height_dimension_dest', \n        'AA1_WTHR_period_quantity_in_hours_dest', 'AA1_WTHR_depth_dimension_dest', \n\n        # esther feature eng\n        'LOCAL_DEP_HOUR', 'HOLIDAY', 'Prev_Flight_Delay_15', 'Enough_Time_Btwn_Estimate_Arrival_and_Planned_Dep', 'Poor_Schedule'\n    ]\n\n    y_var = 'DEP_DEL15_AIRLNS'\n    \n    # create an id column for final join\n    df = df.withColumn(\"xgb_id\", F.monotonically_increasing_id())\n\n    # impute some missing values\n    df2 = df.na.fill({\n        'LATITUDE_WTHR_origin': 0\n        ,'LONGITUDE_WTHR_origin': 0\n        ,'ELEVATION_WTHR_origin': 0\n        ,'LATITUDE_WTHR_dest': 0\n        ,'LONGITUDE_WTHR_dest': 0\n        ,'ELEVATION_WTHR_dest': 0\n        ,'Prev_Flight_Delay_15': 0\n        #,'DEP_DEL15_AIRLNS': 0\n    })\n    \n    # cast some vars to int\n    str_cols = ['Prev_Flight_Delay_15', 'Poor_Schedule', 'Enough_Time_Btwn_Estimate_Arrival_and_Planned_Dep']\n    for column in str_cols:\n        df2 = df2.withColumn(column, F.col(column).cast(types.IntegerType())) \n\n    # vars to index\n    # Specify which columns to index (ie cast to int)\n    vars_to_index = [\n        'ORIGIN_AIRLNS', \n        'DEST_AIRLNS', \n        'OP_UNIQUE_CARRIER_AIRLNS' # a more granular form of origin/dest airlines\n    ]\n\n    # rename cols to drop them later\n    for var in vars_to_index:\n        df2 = df2.withColumnRenamed(var, var+'_old')\n\n    # finally, index them\n    indexer = StringIndexer(inputCols=[i+'_old' for i in vars_to_index], outputCols=vars_to_index)\n    df2 = indexer.fit(df2).transform(df2)\n    df2 = df2.drop(*[i+'_old' for i in vars_to_index])\n    \n    # vectorize\n    df2 = df2.select(X_vars + [y_var])\n    vectorAssembler = VectorAssembler(inputCols = X_vars, outputCol = 'features', handleInvalid='skip')\n    df2 = vectorAssembler.transform(df2).select(['features', y_var])\n    \n    # make predictions\n    df2 = xgb_fit.transform(df2)\n    \n    # Extract probabilities\n    get_item=F.udf(lambda v:float(v[1]), types.FloatType())\n    df2 = df2.withColumn(\"xgb_prob\", get_item('probability'))\n    df2 = df2.withColumnRenamed('prediction', 'xgb_prediction')\n    df2 = df2.select('xgb_prob', 'xgb_prediction')\n    df2 = df2.withColumn('xgb_id', F.monotonically_increasing_id())\n    \n    # join preds to original dataset and return it\n    df = df = df.join(df2, on='xgb_id', how='left').drop('xgb_id')\n    return df\n    \n    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"032ccd20-afa6-45b4-941e-fa6b5a9b4c7d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Demonstration of the algorithm below:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c044015b-f211-4446-8dd2-1745068af88e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["\nblob_container = \"main-storage\" # The name of your container created in https://portal.azure.com\nstorage_account = \"team05w261\" # The name of your Storage account created in https://portal.azure.com\nsecret_scope = \"team05\" # The name of the scope created in your local computer using the Databricks CLI\nsecret_key = \"team05-key\" # The name of the secret key created in your local computer using the Databricks CLI \nblob_url = f\"wasbs://{blob_container}@{storage_account}.blob.core.windows.net\"\nmount_path = \"/mnt/mids-w261\"\nspark.conf.set(\n  f\"fs.azure.account.key.{storage_account}.blob.core.windows.net\",\n  dbutils.secrets.get(scope = secret_scope, key = secret_key)\n)\ndf = spark.read.parquet(f\"{blob_url}/all_time_full_join_6\")\n\ndf_train = xgb_predict(df.filter(F.col('YEAR_AIRLNS')<=2018))\ndf_test = xgb_predict(df.filter(F.col('YEAR_AIRLNS')==2019))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96130cd5-5e43-4a0b-bd8a-87190c38ee31"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# df_test.groupby('xgb_prediction', 'DEP_DEL15_AIRLNS').count().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c3c73aa9-b5f5-4b62-825f-e7a17a3e8831"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------------+----------------+-------+\n|xgb_prediction|DEP_DEL15_AIRLNS|  count|\n+--------------+----------------+-------+\n|           1.0|             1.0| 851342|\n|           0.0|             1.0| 502117|\n|           1.0|             0.0| 933015|\n|           0.0|             0.0|4981758|\n+--------------+----------------+-------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------+----------------+-------+\nxgb_prediction|DEP_DEL15_AIRLNS|  count|\n+--------------+----------------+-------+\n           1.0|             1.0| 851342|\n           0.0|             1.0| 502117|\n           1.0|             0.0| 933015|\n           0.0|             0.0|4981758|\n+--------------+----------------+-------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# +--------------+----------------+-------+\n# |xgb_prediction|DEP_DEL15_AIRLNS|  count|\n# +--------------+----------------+-------+\n# |           1.0|             1.0| 851342|\n# |           0.0|             1.0| 502117|\n# |           1.0|             0.0| 933015|\n# |           0.0|             0.0|4981758|\n# +--------------+----------------+-------+"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f69a4a82-c124-4a72-9fa9-dcaf4d3aacea"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# #f2 score\n# tp = 813227\n# fp = 915754\n# tn = 4999019\n# fn = 540232\n# ((1+2**2) * tp) / ((1+2**2)*tp + 2**2 * fn + fp)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf83b55d-d1ba-4a60-8d3f-a13eb7789b3e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[35]: 0.5692620992529978</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[35]: 0.5692620992529978</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df_test_rdd = df_test.select('xgb_prediction', 'dep_del15_airlns').withColumnRenamed('xgb_prediction', 'prediction').withColumnRenamed('dep_del15_airlns', 'label').rdd\nmetrics = MulticlassMetrics(df_test_rdd)\n    \n# classification metrics\ncm = metrics.confusionMatrix().toArray()\nprint(f'f2 score: {metrics.fMeasure(0.0, 2.0)}')\n\n# confirm I'm getting the same f score here\naccuracy = (cm[0][0] + cm[1][1]) / cm.sum()\nprecision = (cm[1][1]) / (cm[1][1] + cm[0][1])\nrecall = (cm[1][1]) / (cm[1][1] + cm[1][0])\n\ndef f_score(beta, precision, recall):\n    return (1+beta**2) * precision * recall / (beta**2 * precision + recall)\n\nprint(f'f2 score calc2: {f_score(2, precision, recall)}')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e7c5260-e6e4-47fd-840b-eaf647be32c6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">f2 score: 0.8547101604308168\nf2 score calc2: 0.591358136687916\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">f2 score: 0.8547101604308168\nf2 score calc2: 0.591358136687916\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n\npredictions = df_train\\\n    .withColumnRenamed('xgb_prob', 'probability')\\\n    .withColumnRenamed('xgb_prediction', 'rawPrediction')\\\n    .withColumnRenamed('DEP_DEL15_AIRLNS', 'label')\\\n    .select('probability', 'label', 'rawPrediction')\\\n    .filter(F.col('label').isNotNull())\n\nevaluator = BinaryClassificationEvaluator(labelCol='label')\n\n# We have only two choices: area under ROC and PR curves :-(\nauroc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\nauprc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})\nprint(\"Area under ROC Curve: {:.4f}\".format(auroc))\nprint(\"Area under PR Curve: {:.4f}\".format(auprc))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"997990a2-e43b-48b1-8bf2-eb023a282139"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f80ee9db-460c-4b4d-acef-fbd00fd2aff6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"207aafcd-4a23-46a4-bb78-89b50467ff55"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46796f36-a15a-46bd-8a75-35626a675862"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"xgb_predict_function","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1858507102372439}},"nbformat":4,"nbformat_minor":0}
